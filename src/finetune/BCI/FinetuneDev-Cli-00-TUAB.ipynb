{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b261b003-fa6a-492c-a57a-c02b5da2d6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recomputing durations:   0%|             | 178/69652 [02:11<14:14:52,  1.35it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import mne\n",
    "mne.set_log_level(\"WARNING\")\n",
    "\n",
    "def get_generic_channel_name(self, channel_name):\n",
    "    channel_name = channel_name.lower()\n",
    "    # Remove \"eeg \" prefix if present\n",
    "    if channel_name.startswith(\"eeg \"):\n",
    "        channel_name = channel_name[4:]\n",
    "    # Simplify names with a dash and check if it ends with \"-\"\n",
    "    if \"-\" in channel_name:\n",
    "        if channel_name.endswith(\"-\"):\n",
    "            return \"None\"\n",
    "        return channel_name.split(\"-\")[0]\n",
    "    return channel_name\n",
    "\n",
    "def create_raw(\n",
    "    data,\n",
    "    ch_names1,\n",
    "    sr,\n",
    "    ch_names2=None,\n",
    "):\n",
    "    if ch_names2 == None:\n",
    "        ch_names2 = ch_names1\n",
    "    ch_types = [\"eeg\" for _ in range(len(ch_names1))]\n",
    "    info = mne.create_info(ch_names2, ch_types=ch_types, sfreq=sr)\n",
    "    eeg_data = np.array(data[ch_names1].T, dtype=\"float\") / 1_000_000\n",
    "    raw = mne.io.RawArray(eeg_data, info)\n",
    "    return raw\n",
    "\n",
    "def avg_channel(raw):\n",
    "    avg = raw.copy().add_reference_channels(ref_channels=\"AVG_REF\")\n",
    "    avg = avg.set_eeg_reference(ref_channels=\"average\")\n",
    "    return avg\n",
    "\n",
    "def load_from_path(path, channels, sr):\n",
    "    if path.endswith(\"edf\"):\n",
    "        eeg_data = mne.io.read_raw_edf(\n",
    "            path,\n",
    "            include=channels,\n",
    "            preload=True,\n",
    "        )\n",
    "    elif path.endswith(\"pkl\"):\n",
    "        # Load DataFrame from pickle\n",
    "        with open(path, \"rb\") as file:\n",
    "            df = pd.read_pickle(file)\n",
    "            eeg_data = create_raw(\n",
    "                data=df,\n",
    "                ch_names1=channels,\n",
    "                sr=sr,\n",
    "            )\n",
    "    else:\n",
    "        assert False, \"Invalid path\"\n",
    "\n",
    "    # Add average reference\n",
    "    eeg_data = avg_channel(eeg_data)\n",
    "\n",
    "    # Datastructure to access data for each channel\n",
    "    channel_data_dict = {}\n",
    "\n",
    "    # Note: channel_data_dict also includes the AVG_REF channel\n",
    "    for channel in eeg_data.ch_names:\n",
    "        idx = eeg_data.ch_names.index(channel)\n",
    "        data, times = eeg_data[idx, :]\n",
    "        # Flatten the data to 1D if required\n",
    "        channel_data_dict[channel] = data.flatten()\n",
    "\n",
    "    df = pd.DataFrame(channel_data_dict)\n",
    "    df[\"Time in Seconds\"] = times.flatten()\n",
    "\n",
    "    return df\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tueg_path_new = \"/itet-stor/maxihuber/deepeye_storage/index_files/new_tueg_index.json\"\n",
    "\n",
    "with open(\"/itet-stor/maxihuber/deepeye_storage/index_files/full_tueg_index.json\", \"r\") as f:\n",
    "    index = json.load(f)\n",
    "\n",
    "prefix_path = \"/itet-stor/maxihuber/deepeye_storage/foundation/tueg/edf\"\n",
    "\n",
    "for i, ie in enumerate(tqdm(index, \"Recomputing durations\", position=0, leave=True)):\n",
    "    file = prefix_path + ie[\"path\"]\n",
    "    # load\n",
    "    df = load_from_path(file, ie[\"channels\"], ie[\"sr\"])\n",
    "    ie[\"duration\"] = len(df) / ie[\"sr\"]\n",
    "    del df\n",
    "\n",
    "    if i % 1_000 == 0:\n",
    "        with open(tueg_path_new, 'w') as f:\n",
    "            json.dump(index, f, indent=4)\n",
    "\n",
    "with open(tueg_path_new, 'w') as f:\n",
    "    json.dump(index, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa235da2-5ab9-48b4-856d-af5f05b12d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '21', '22', '24', '26', '28', '30', '31', '35', '36', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '56', '57', '58', '59', '62', '63', '65', '66', '68', '69', '70', '73', '77', '78', '79', '81', '82', '85', '86', '87', '88', '89', '90', '91', '92', '94', '98', '99', '100', '103', '105', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '123', '127', '128', 'a1', 'a2', 'c3', 'c3p', 'c4', 'c4p', 'cz', 'f3', 'f4', 'f7', 'f8', 'fp1', 'fp2', 'fz', 'o1', 'o2', 'oz', 'p3', 'p4', 'pz', 't1', 't2', 't3', 't4', 't5', 't6']\n",
      "{'None': 0, 'a1': 1, 'a2': 2, 'c3': 3, 'c3p': 4, 'c4p': 5, 'cz': 6, 'f3': 7, 'f4': 8, 'f7': 9, 'f8': 10, 'fp1': 11, 'fp2': 12, 'fz': 13, 'o1': 14, 'o2': 15, 'oz': 16, 'p3': 17, 'p4': 18, 'pz': 19, 't1': 20, 't2': 21, 't3': 22, 't4': 23, 't5': 24, 't6': 25}\n",
      "{'None': 0, 'a1': 1, 'a2': 2, 'c3': 3, 'c3p': 4, 'c4p': 5, 'cz': 6, 'f3': 7, 'f4': 8, 'f7': 9, 'f8': 10, 'fp1': 11, 'fp2': 12, 'fz': 13, 'o1': 14, 'o2': 15, 'oz': 16, 'p3': 17, 'p4': 18, 'pz': 19, 't1': 20, 't2': 21, 't3': 22, 't4': 23, 't5': 24, 't6': 25}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "break before dumping",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(real_chns)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(high_counts)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbreak before dumping\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/maxihuber/eeg-foundation/src/data/components/channels_to_id2.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     53\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(high_counts, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: break before dumping"
     ]
    }
   ],
   "source": [
    "from natsort import natsorted\n",
    "\n",
    "with open(\"/itet-stor/maxihuber/deepeye_storage/index_files/full_tueg_index.json\", \"r\") as f:\n",
    "    tueg = json.load(f)\n",
    "\n",
    "with open(\"/itet-stor/maxihuber/deepeye_storage/index_files/tuab_train_index.json\", \"r\") as f:\n",
    "    tuab = json.load(f)\n",
    "\n",
    "full = tueg + tuab\n",
    "\n",
    "chns = set()\n",
    "for ie in full:\n",
    "    chns.update([get_generic_channel_name(chn) for chn in ie[\"channels\"]])\n",
    "\n",
    "real_chns = ['f8', 't3', 't6', 'f3', 'fp2', 't1', 'f4', 'o1', 'fp1', 't4', 'p3', 'cz', 't5', 'c3', 'p4', 'oz', 'f7', 'fz', 'c4p', 'c3p', 'o2', 'pz', 't2', 'a1', 'a2']\n",
    "real_chns = natsorted(real_chns)\n",
    "real_chns = ['None'] + real_chns\n",
    "real_chns = {v: k for k, v in enumerate(real_chns)}\n",
    "\n",
    "all_real_chns = set(['105', 't6', '47', '44', '28', 't3', '103', '100', '82', '81', '45', 'p3', '78', '77', '50', 'p4', 'f4', 'f8', '20', '99', '98', 'f3', 'f7', '57', '56', '36', '35', '66', '65', '63', '62', '31', '30', '69', '68', '107', '70', '120', 'a2', 't1', 'a1', '79', '127', '128', '26', '24', '121', '22', '21', '123', '85', 't2', 'c4p', '89', '88', '87', '86', '94', '92', '91', '90', '109', '108', '111', '110', '113', '112', '115', '114', '117', '116', '119', '118', '42', '59', '58', '41', '40', '43', '42', 'o1', '48', 'c3', 't5', '46', '49', 'c3p', 'cz', 'c4', 't4', '73', 'c3', 't1', 't2', 't3', 't4', 't5', 't6', 'p3', 'p4', 'pz', 'f7', 'f8', 'f3', 'f4', 'fz', 'o1', 'o2', 'oz', 'fp1', 'fp2'])\n",
    "all_real_chns = set(natsorted(list(all_real_chns)))\n",
    "print(all_real_chns)\n",
    "\n",
    "counts = {}\n",
    "for ie in full:\n",
    "    good_channels = []\n",
    "    for chn in ie[\"channels\"]:\n",
    "        if get_generic_channel_name(chn) in all_real_chns:\n",
    "            good_channels.append(chn)\n",
    "        counts[chn] = counts[chn] + 1 if chn in counts else 1\n",
    "    ie[\"good_channels\"] = good_channels\n",
    "\n",
    "high_counts = {'fp1': 72369, 'fp2': 72369, 'f3': 72368, 'f4': 72369, \n",
    "               'c3': 72371, 'c4': 72371, 'p3': 72367, 'p4': 72367, \n",
    "               'o1': 72367, 'o2': 72367, 'f7': 72369, 'f8': 72369, \n",
    "               't3': 72371, 't4': 72371, 't5': 72369, 't6': 72369, \n",
    "               'a1': 65950, 'a2': 65950, 'fz': 72130, 'cz': 72370, \n",
    "               'pz': 72128, 'roc': 26181, 'loc': 26180, 'ekg1': 56335, \n",
    "               't1': 61068, 't2': 61072, '26': 21088, '27': 20471, \n",
    "               '28': 22780, '29': 24374, '30': 25998, 'oz': 12864, \n",
    "               'pg1': 12240, 'pg2': 12240, 'ekg': 13358, '31': 29457, '32': 29457, \n",
    "               '23': 3179, '24': 3179, '20': 2599, '21': 2626, '22': 2626, \n",
    "               '25': 4790, 'c3p': 23616, 'c4p': 23613, 'sp1': 29445, 'sp2': 28601}\n",
    "high_counts = list(high_counts.keys())\n",
    "high_counts = list(set(high_counts) & set(real_chns.keys()))\n",
    "high_counts = natsorted(high_counts)\n",
    "high_counts = ['None'] + high_counts\n",
    "high_counts = {v: k for k, v in enumerate(high_counts)}\n",
    "\n",
    "print(real_chns)\n",
    "print(high_counts)\n",
    "\n",
    "assert False, \"break before dumping\"\n",
    "\n",
    "with open('/home/maxihuber/eeg-foundation/src/data/components/channels_to_id2.json', 'w') as f:\n",
    "    json.dump(high_counts, f, indent=4)\n",
    "    print(\"Dumped file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6665b7c2-3272-4360-9f59-d17af6bf2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/itet-stor/maxihuber/deepeye_storage/index_files/311G_tueg_index.json\", \"r\") as f:\n",
    "    tueg = json.load(f)\n",
    "\n",
    "with open(\"/itet-stor/maxihuber/deepeye_storage/index_files/tuab_train_index2.json\", \"r\") as f:\n",
    "    tuab_train = json.load(f)\n",
    "\n",
    "with open(\"/itet-stor/maxihuber/deepeye_storage/index_files/tuab_test_index2.json\", \"r\") as f:\n",
    "    tuab_test = json.load(f)\n",
    "\n",
    "index = tueg\n",
    "\n",
    "for ie in index:\n",
    "    good_channels = []\n",
    "    for chn in ie[\"channels\"]:\n",
    "        if get_generic_channel_name(chn) in all_real_chns:\n",
    "            good_channels.append(chn)\n",
    "    ie[\"good_channels\"] = good_channels\n",
    "\n",
    "with open(\"/itet-stor/maxihuber/deepeye_storage/index_files/311G_tueg_index2.json\", \"w\") as f:\n",
    "    json.dump(index, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "74455caf-5cbb-47bc-81c9-e5deb88bc4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/itet-stor/maxihuber/deepeye_storage/index_files/full_tueg_index.json', 'r') as f:\n",
    "    tueg = json.load(f)\n",
    "\n",
    "with open('/itet-stor/maxihuber/deepeye_storage/index_files/311G_tueg_index.json', 'r') as f:\n",
    "    index300 = json.load(f)\n",
    "\n",
    "# Convert tueg from list to dictionary for faster searching access\n",
    "tueg_dict = {entry['path']: entry for entry in tueg}  # Assuming each entry has a unique 'id' key\n",
    "\n",
    "index300new = []\n",
    "for ie in index300:\n",
    "    # Find the same ie in tueg_dict\n",
    "    ie_id = ie['path']  # Assuming each entry in index300 has an 'id' key\n",
    "    if ie_id in tueg_dict:\n",
    "        ie_tueg = tueg_dict[ie_id]\n",
    "        index300new.append(ie_tueg)\n",
    "\n",
    "with open('/itet-stor/maxihuber/deepeye_storage/index_files/311G_tueg_index2.json', 'w') as f:\n",
    "    json.dump(index300new, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "23925e01-a24e-42de-949f-7f42ac2c0d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_id(filepath):\n",
    "    if filepath.endswith(\"pkl\"):\n",
    "        # Regular expression to match the UUID in the middle of the file path\n",
    "        match = re.search(\n",
    "            r\"[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}\", filepath\n",
    "        )\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "        return None\n",
    "    elif filepath.endswith(\"edf\"):\n",
    "        parts = filepath.split(\"/\")\n",
    "        subject_id = parts[\n",
    "            2\n",
    "        ]  # The subject ID is the third element after splitting by '/'\n",
    "        return subject_id\n",
    "    else:\n",
    "        assert False, f\"invaliv file format for file {filepath}\"\n",
    "\n",
    "with open(\"/itet-stor/maxihuber/deepeye_storage/index_files/tuab_test_index.json\", \"r\") as f:\n",
    "    tuab = json.load(f)\n",
    "\n",
    "for ie in tuab:\n",
    "    ie[\"ref\"] = None\n",
    "    ie[\"SubjectID\"] = get_subject_id(ie[\"path\"])\n",
    "    ie[\"Dataset\"] = \"TUAB\"\n",
    "\n",
    "assert False, \"break before dumping\"\n",
    "\n",
    "with open(\"/itet-stor/maxihuber/deepeye_storage/index_files/tuab_test_index2.json\", \"w\") as f:\n",
    "    tuab = json.dump(tuab, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "094b99e4-9446-4f08-9626-e77f08986678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "print(len('/itet-stor/maxihuber/deepeye_storage/foundation/tueg/edf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71b82d11-f3c0-470c-8a00-c838e24e826e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ie \u001b[38;5;129;01min\u001b[39;00m index:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chn \u001b[38;5;129;01min\u001b[39;00m ie[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 16\u001b[0m         chn \u001b[38;5;241m=\u001b[39m \u001b[43mget_generic_channel_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chn \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m23\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m, in \u001b[0;36mget_generic_channel_name\u001b[0;34m(channel_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Simplify names with a dash and check if it ends with \"-\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m channel_name:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mchannel_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m channel_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_generic_channel_name(channel_name):\n",
    "    channel_name = channel_name.lower()\n",
    "    # Remove \"eeg \" prefix if present\n",
    "    if channel_name.startswith(\"eeg \"):\n",
    "        channel_name = channel_name[4:]\n",
    "    # Simplify names with a dash and check if it ends with \"-\"\n",
    "    if \"-\" in channel_name:\n",
    "        if channel_name.endswith(\"-\"):\n",
    "            return \"None\"\n",
    "        return channel_name.split(\"-\")[0]\n",
    "    return channel_name\n",
    "\n",
    "hist = {}\n",
    "for ie in index:\n",
    "    for chn in ie[\"channels\"]:\n",
    "        chn = get_generic_channel_name(chn)\n",
    "        if chn == '23':\n",
    "            pass\n",
    "            #print(ie[\"sr\"])\n",
    "        hist[chn] = hist[chn]+1 if chn in hist else 1\n",
    "\n",
    "durs = [int(ie[\"duration\"]) for ie in index if ie[\"duration\"] > 10]\n",
    "counts, bins, _ = plt.hist(durs, bins=range(min(durs), max(durs) + 3_600, 3_600))\n",
    "\n",
    "for bin, count in zip(bins, counts):\n",
    "    print(f\"{bin}: {count}\")\n",
    "#print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9b1a80-cd86-4ed3-b427-f038f1e0601a",
   "metadata": {},
   "source": [
    "# Finetuning Notebook for NeuroBench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f2d7d-b21d-4383-ac28-f6f3616c10aa",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13f91bcd-d30d-4054-a2d1-76b9b3d13c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_nr_x_patches(win_size, dur):\n",
    "    win_shift = win_size * self_win_shift_factor\n",
    "    x_datapoints_per_second = 1 / win_shift\n",
    "    x_datapoints = dur * x_datapoints_per_second + 1\n",
    "    return int(x_datapoints // self_patch_size)\n",
    "\n",
    "get_nr_x_patches(.25, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f7d5add-ea2e-4f05-99c9-6a92e22c015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/itet-stor/maxihuber/deepeye_storage/foundation/tueg_tasks/epilepsy/00_epilepsy/aaaaamoa/s004_2012/01_tcp_ar/aaaaamoa_s004_t000.edf\"\n",
    "\n",
    "def get_generic_channel_name(channel_name):\n",
    "    channel_name = channel_name.lower()\n",
    "    # Remove \"eeg \" prefix if present\n",
    "    if channel_name.startswith(\"eeg \"):\n",
    "        channel_name = channel_name[4:]\n",
    "    # Simplify names with a dash and check if it ends with \"-\"\n",
    "    if \"-\" in channel_name:\n",
    "        if channel_name.endswith(\"-\"):\n",
    "            return \"None\"\n",
    "        return channel_name.split(\"-\")[0]\n",
    "    return channel_name\n",
    "\n",
    "def load_edf_to_dataframe(file_path):\n",
    "    eeg_data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    channel_data_dict = {}\n",
    "\n",
    "    for channel in eeg_data.ch_names:\n",
    "        idx = eeg_data.ch_names.index(channel)\n",
    "        channel = get_generic_channel_name(channel)\n",
    "        data, times = eeg_data[idx, :]\n",
    "        channel_data_dict[channel] = data.flatten()\n",
    "\n",
    "    df = pd.DataFrame(channel_data_dict)\n",
    "    df['Time in Seconds'] = times.flatten()\n",
    "    return df\n",
    "\n",
    "df = load_edf_to_dataframe(file)\n",
    "task_channels = set(df.columns) & set(['p4', 'c3', 'pz', 'fp2', 't6', 'fz', 'f4', 'f8', 't4', 't3', 'p3', 'a2', 'oz', 'a1', 't5', 't1', 'cz', 'c4', 'fp1', 'o1', 'o2', 'f3', 'f7']))\n",
    "\n",
    "signal = df[\"cz\"] * 1_000_000\n",
    "signal = torch.tensor(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae09f0dd-fa3e-41d5-af4e-2e1e337f50ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "{'t5', 'p4', 'o1', 't1', 'fp2', 'fz', 'cz', 't3', 'a1', 't6', 'f3', 'c4', 'f7', 'o2', 'a2', 'c3', 'p3', 'f8', 'f4', 't4', 'fp1', 'pz'}\n"
     ]
    }
   ],
   "source": [
    "task_channels = set(df.columns) & set(['p4', 'c3', 'pz', 'fp2', 't6', 'fz', 'f4', 'f8', 't4', 't3', 'p3', 'a2', 'oz', 'a1', 't5', 't1', 'cz', 'c4', 'fp1', 'o1', 'o2', 'f3', 'f7'])\n",
    "print(len(task_channels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "955a5697-393f-4fdd-8213-64598d1319da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A1', 'A2', 'AF1', 'AF10', 'AF10h', 'AF1h', 'AF2', 'AF2h', 'AF3', 'AF3h', 'AF4', 'AF4h', 'AF5', 'AF5h', 'AF6', 'AF6h', 'AF7', 'AF7h', 'AF8', 'AF8h', 'AF9', 'AF9h', 'AFF1', 'AFF10', 'AFF10h', 'AFF1h', 'AFF2', 'AFF2h', 'AFF3', 'AFF3h', 'AFF4', 'AFF4h', 'AFF5', 'AFF5h', 'AFF6', 'AFF6h', 'AFF7', 'AFF7h', 'AFF8', 'AFF8h', 'AFF9', 'AFF9h', 'AFFz', 'AFp1', 'AFp10', 'AFp10h', 'AFp1h', 'AFp2', 'AFp2h', 'AFp3', 'AFp3h', 'AFp4', 'AFp4h', 'AFp5', 'AFp5h', 'AFp6', 'AFp6h', 'AFp7', 'AFp7h', 'AFp8', 'AFp8h', 'AFp9', 'AFp9h', 'AFpz', 'AFz', 'C1', 'C1h', 'C2', 'C2h', 'C3', 'C3h', 'C4', 'C4h', 'C5', 'C5h', 'C6', 'C6h', 'CCP1', 'CCP1h', 'CCP2', 'CCP2h', 'CCP3', 'CCP3h', 'CCP4', 'CCP4h', 'CCP5', 'CCP5h', 'CCP6', 'CCP6h', 'CCPz', 'CP1', 'CP1h', 'CP2', 'CP2h', 'CP3', 'CP3h', 'CP4', 'CP4h', 'CP5', 'CP5h', 'CP6', 'CP6h', 'CPP1', 'CPP1h', 'CPP2', 'CPP2h', 'CPP3', 'CPP3h', 'CPP4', 'CPP4h', 'CPP5', 'CPP5h', 'CPP6', 'CPP6h', 'CPPz', 'CPz', 'Cz', 'F1', 'F10', 'F10h', 'F1h', 'F2', 'F2h', 'F3', 'F3h', 'F4', 'F4h', 'F5', 'F5h', 'F6', 'F6h', 'F7', 'F7h', 'F8', 'F8h', 'F9', 'F9h', 'FC1', 'FC1h', 'FC2', 'FC2h', 'FC3', 'FC3h', 'FC4', 'FC4h', 'FC5', 'FC5h', 'FC6', 'FC6h', 'FCC1', 'FCC1h', 'FCC2', 'FCC2h', 'FCC3', 'FCC3h', 'FCC4', 'FCC4h', 'FCC5', 'FCC5h', 'FCC6', 'FCC6h', 'FCCz', 'FCz', 'FFC1', 'FFC1h', 'FFC2', 'FFC2h', 'FFC3', 'FFC3h', 'FFC4', 'FFC4h', 'FFC5', 'FFC5h', 'FFC6', 'FFC6h', 'FFCz', 'FFT10', 'FFT10h', 'FFT7', 'FFT7h', 'FFT8', 'FFT8h', 'FFT9', 'FFT9h', 'FT10', 'FT10h', 'FT7', 'FT7h', 'FT8', 'FT8h', 'FT9', 'FT9h', 'FTT10', 'FTT10h', 'FTT7', 'FTT7h', 'FTT8', 'FTT8h', 'FTT9', 'FTT9h', 'Fp1', 'Fp1h', 'Fp2', 'Fp2h', 'Fpz', 'Fz', 'I1', 'I1h', 'I2', 'I2h', 'Iz', 'M1', 'M2', 'O1', 'O1h', 'O2', 'O2h', 'OI1', 'OI1h', 'OI2', 'OI2h', 'OIz', 'Oz', 'P1', 'P10', 'P10h', 'P1h', 'P2', 'P2h', 'P3', 'P3h', 'P4', 'P4h', 'P5', 'P5h', 'P6', 'P6h', 'P7', 'P7h', 'P8', 'P8h', 'P9', 'P9h', 'PO1', 'PO10', 'PO10h', 'PO1h', 'PO2', 'PO2h', 'PO3', 'PO3h', 'PO4', 'PO4h', 'PO5', 'PO5h', 'PO6', 'PO6h', 'PO7', 'PO7h', 'PO8', 'PO8h', 'PO9', 'PO9h', 'POO1', 'POO10', 'POO10h', 'POO1h', 'POO2', 'POO2h', 'POO3', 'POO3h', 'POO4', 'POO4h', 'POO5', 'POO5h', 'POO6', 'POO6h', 'POO7', 'POO7h', 'POO8', 'POO8h', 'POO9', 'POO9h', 'POOz', 'POz', 'PPO1', 'PPO10', 'PPO10h', 'PPO1h', 'PPO2', 'PPO2h', 'PPO3', 'PPO3h', 'PPO4', 'PPO4h', 'PPO5', 'PPO5h', 'PPO6', 'PPO6h', 'PPO7', 'PPO7h', 'PPO8', 'PPO8h', 'PPO9', 'PPO9h', 'PPOz', 'Pz', 'T10', 'T10h', 'T3', 'T4', 'T5', 'T6', 'T7', 'T7h', 'T8', 'T8h', 'T9', 'T9h', 'TP10', 'TP10h', 'TP7', 'TP7h', 'TP8', 'TP8h', 'TP9', 'TP9h', 'TPP10', 'TPP10h', 'TPP7', 'TPP7h', 'TPP8', 'TPP8h', 'TPP9', 'TPP9h', 'TTP10', 'TTP10h', 'TTP7', 'TTP7h', 'TTP8', 'TTP8h', 'TTP9', 'TTP9h']\n"
     ]
    }
   ],
   "source": [
    "montage = mne.channels.make_standard_montage('standard_1005')\n",
    "print(sorted(montage.ch_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec7e8af-477d-4d7a-b00d-4c6a7da090ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/maxihuber/eeg-foundation/\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import lightning as L\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import random\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import mne\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from mne.preprocessing import Xdawn\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, mean_squared_error\n",
    "\n",
    "import torchaudio\n",
    "from src.data.transforms import (\n",
    "    crop_spg,\n",
    "    custom_fft,\n",
    "    normalize_spg,\n",
    ")\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchmetrics\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from src.models.mae_rope_encoder import EncoderViTRoPE\n",
    "from src.models.components.vit_rope import (\n",
    "    Flexible_RoPE_Layer_scale_init_Block,\n",
    "    FlexibleRoPEAttention,\n",
    "    compute_axial_cis,\n",
    "    select_freqs_cis,\n",
    ")\n",
    "from timm.models.vision_transformer import Mlp as Mlp\n",
    "from torch.nn import TransformerEncoderLayer\n",
    "from src.models.components.SimpleTransformer import SimpleTransformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from torchmetrics.functional import mean_squared_error as rmse\n",
    "import lightning.pytorch as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from collections import defaultdict\n",
    "\n",
    "mne.set_log_level('warning')\n",
    "\n",
    "L.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea036b9-4979-4883-8acc-708017b99fb1",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3428f8de-eb21-492f-b8fd-a274c9a8d86a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load Train/Val/Test Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140a137e-6e06-483e-8ff6-f9722725fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# TUAB and Epilepsy\n",
    "\n",
    "yc_class = {\n",
    "    \"class_name\": \"YC\",\n",
    "    \"time_col\": \"Time in Seconds\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation/tueg/edf\",\n",
    "    \"load_mode\": 2,\n",
    "}\n",
    "\n",
    "tuab = {\n",
    "    \"task_name\": \"TUAB\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/cli/tuab.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "epilepsy = {\n",
    "    \"task_name\": \"Epilepsy\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/cli/epilepsy.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "########################################################################################################################\n",
    "# Clinical JSONs\n",
    "\n",
    "cli_class = {\n",
    "    \"class_name\": \"Clinical\",\n",
    "    \"time_col\": \"Time in Seconds\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_prepared/\",\n",
    "    \"load_mode\": 0,\n",
    "}\n",
    "\n",
    "age = {\n",
    "    \"task_name\": \"Age\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/age.json\",\n",
    "    \"out_dim\": 1,\n",
    "}\n",
    "\n",
    "depression = {\n",
    "    \"task_name\": \"Depression\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/cli_depression.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "parkinsons = {\n",
    "    \"task_name\": \"Parkinsons\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/cli_parkinsons.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "schizophrenia = {\n",
    "    \"task_name\": \"Schizophrenia\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/cli_schizophrenia.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "sex = {\n",
    "    \"task_name\": \"Sex\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/sex.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# Motor-Imagery JSONs\n",
    "\n",
    "mi_class = {\n",
    "    \"class_name\": \"Motor Imagery\",\n",
    "    \"time_col\": \"time in seconds\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation_prepared/\",\n",
    "    \"load_mode\": 0,\n",
    "}\n",
    "\n",
    "eye_open_closed = {\n",
    "    \"task_name\": \"EyeOpenClosed\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/eye_open_closed.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"eye open\", \"eye closed\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "eye_vh = {\n",
    "    \"task_name\": \"EyeVH\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/eye_vh.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"vertical\", \"horizontal\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "flexion_extension_imaginary = {\n",
    "    \"task_name\": \"FlexionExtensionImaginary\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/flexion_extension_imaginary.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"hand movement imagined elbow flexion\",\n",
    "            \"hand movement imagined elbow extension\",\n",
    "        ]\n",
    "    ),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "flexion_extension_real = {\n",
    "    \"task_name\": \"FlexionExtensionReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/flexion_extension_real.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"hand movement elbow extension\", \"hand movement elbow flexion\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "grasp_imaginary = {\n",
    "    \"task_name\": \"GraspImaginary\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/grasp_imaginary.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"imagined palmar grasp\", \"imagined lateral grasp\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "grasp_real = {\n",
    "    \"task_name\": \"GraspReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/grasp_real.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"movement palmar grasp\", \"movement lateral grasp\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "lr_imaginary = {\n",
    "    \"task_name\": \"LRImaginary\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/lr_imaginary.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"left hand imagined movement\", \"right hand imagined movement\"]),\n",
    "    \"short_mode\": True,\n",
    "}\n",
    "\n",
    "lr_real = {\n",
    "    \"task_name\": \"LRReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/lr_real.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"right hand movement\", \"left hand movement\"]),\n",
    "    \"short_mode\": True,\n",
    "}\n",
    "\n",
    "mi_task_body_parts_real = {\n",
    "    \"task_name\": \"BodyPartsReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/mi_task_body_parts.json\",\n",
    "    \"out_dim\": 4,\n",
    "    \"outputs\": set(\n",
    "        [\"rest\", \"right hand movement\", \"foot movement\", \"left hand movement\"]\n",
    "    ),\n",
    "    \"short_mode\": True,\n",
    "}\n",
    "\n",
    "mi_task_body_parts_imagined = {\n",
    "    \"task_name\": \"BodyPartsImagined\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/mi_task_body_parts.json\",\n",
    "    \"out_dim\": 4,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"rest\",\n",
    "            \"right hand imagined movement\",\n",
    "            \"foot imagined movement\",\n",
    "            \"left hand imagined movement\",\n",
    "            \"tongue imagined movement\",\n",
    "        ]\n",
    "    ),\n",
    "    \"short_mode\": True,\n",
    "}\n",
    "\n",
    "pronation_supination_real = {\n",
    "    \"task_name\": \"PronationSupinationReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/pronation_supination_real.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"movement supination\", \"movement pronation\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "pronation_supination_imaginary = {\n",
    "    \"task_name\": \"PronationSupinationImaginary\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/pronation_supination_imaginary.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"imagined supination\", \"imagined pronation\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "########################################################################################################################\n",
    "# ERP JSONs\n",
    "\n",
    "erp_class = {\n",
    "    \"class_name\": \"Error-Related Potential\",\n",
    "    \"time_col\": \"time in seconds\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation_prepared/\",\n",
    "    \"load_mode\": 0,\n",
    "}\n",
    "\n",
    "erp = {\n",
    "    \"task_name\": \"ERP\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/erp/erp_all.json\",\n",
    "    \"out_dim\": 5,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"Participant is in resting state\",\n",
    "            \"with event-related potential\",\n",
    "            \"Participant is in interval between two flashes\",\n",
    "            \"without event-related potential\",\n",
    "            \"Participant keeps closing eyes\",\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "errp = {\n",
    "    \"task_name\": \"ERRP\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/erp/errp_all.json\",\n",
    "    \"out_dim\": 7,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"Target is located in the right\",\n",
    "            \"without error-related potential\",\n",
    "            \"The cursor moves to the left\",\n",
    "            \"The feedback consisted in the selected item is presented on the screen\",\n",
    "            \"The cursor moves to the right\",\n",
    "            \"with error-related potential\",\n",
    "            \"Target is located in the left\",\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "########################################################################################################################\n",
    "# EyeNet JSONs\n",
    "\n",
    "eye_class = {\n",
    "    \"class_name\": \"EyeNet\",\n",
    "    \"time_col\": \"time\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation_prepared/\",\n",
    "    \"load_mode\": 1,\n",
    "}\n",
    "\n",
    "eye_dir_amp = {\n",
    "    \"task_name\": \"EyeNetDirectionAmp\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": [\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Amp_train.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Amp_val.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Amp_test.json\",\n",
    "    ],\n",
    "    \"out_dim\": 1,\n",
    "}\n",
    "\n",
    "eye_dir_ang = {\n",
    "    \"task_name\": \"EyeNetDirectionAng\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": [\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Ang_train.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Ang_val.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Ang_test.json\",\n",
    "    ],\n",
    "    \"out_dim\": 1,\n",
    "}\n",
    "\n",
    "eye_lr = {\n",
    "    \"task_name\": \"EyeNetLR\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": [\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_LR_train.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_LR_val.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_LR_test.json\",\n",
    "    ],\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "eye_position = {\n",
    "    \"task_name\": \"EyeNetPosition\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": [\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Position_train.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Position_val.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Position_test.json\",\n",
    "    ],\n",
    "    \"out_dim\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b6468-0350-4044-99ba-3683577cd59b",
   "metadata": {},
   "source": [
    "### Load data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc2ba553-0c71-4133-9175-500d9b25fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/itet-stor/maxihuber/deepeye_storage/foundation_tasks/cli/tuab.json\n",
      "Full train size: 2717\n",
      "Full test size: 276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Load train data====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:49<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Load test data====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:19<00:00,  5.07it/s]\n",
      "Train classes: {'normal', 'abnormal'}\n",
      "Test classes: {'normal', 'abnormal'}\n",
      "Full Output Counts: defaultdict(<class 'int'>, {1: 150, 0: 150})\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# Select the class and task\n",
    "\n",
    "used_class = yc_class\n",
    "# used_class = cli_class\n",
    "# used_class = mi_class\n",
    "# used_class = erp_class\n",
    "# used_class = eye_class\n",
    "#\n",
    "used_task = tuab\n",
    "# used_task = epilepsy\n",
    "# used_task = age\n",
    "# used_task = depression\n",
    "# used_task = parkinsons\n",
    "# used_task = schizophrenia\n",
    "# used_task = sex\n",
    "#\n",
    "# used_task = eye_open_closed\n",
    "# used_task = eye_vh\n",
    "# used_task = flexion_extension_imaginary\n",
    "# used_task = flexion_extension_real\n",
    "# used_task = grasp_real\n",
    "# used_task = lr_imaginary\n",
    "# used_task = lr_real\n",
    "# used_task = mi_task_body_parts_real\n",
    "# used_task = mi_task_body_parts_imagined\n",
    "# used_task = pronation_supination_real\n",
    "# used_task = pronation_supination_imaginary\n",
    "#\n",
    "# used_task = erp\n",
    "# used_task = errp\n",
    "#\n",
    "# used_task = eye_dir_amp\n",
    "# used_task = eye_dir_ang\n",
    "# used_task = eye_lr\n",
    "# used_task = eye_position\n",
    "\n",
    "class_name = used_class[\"class_name\"]\n",
    "time_col = used_class[\"time_col\"]\n",
    "prefix_filepath = used_class[\"prefix_filepath\"]\n",
    "load_mode = used_class[\"load_mode\"]\n",
    "task_name = used_task[\"task_name\"]\n",
    "task_type = used_task[\"task_type\"]\n",
    "json_path = used_task[\"json_path\"]\n",
    "out_dim = used_task[\"out_dim\"]\n",
    "short_mode = used_task[\"short_mode\"] if \"short_mode\" in used_task else False\n",
    "\n",
    "task_channels = set(['p4', 'c3', 'pz', 'fp2', 't6', 'fz', 'f4', 'f8', 't4', 't3', 'p3', 'a2', 'oz', 'a1', 't5', 't1', 'cz', 'c4', 'fp1', 'o1', 'o2', 'f3', 'f7'])\n",
    "\n",
    "\n",
    "def load_index0(data_index_path):\n",
    "    with open(data_index_path, \"r\") as f:\n",
    "        train_test_dict = json.load(f)\n",
    "    train_samples = train_test_dict[\"train\"]\n",
    "    test_samples = train_test_dict[\"test\"]\n",
    "    return train_samples, test_samples\n",
    "\n",
    "\n",
    "def load_index1(data_index_paths):\n",
    "    all_samples = []\n",
    "    for data_index_path in data_index_paths:\n",
    "        with open(data_index_path, \"r\") as f:\n",
    "            subset_dict = json.load(f)\n",
    "        all_samples.append(list(subset_dict.values())[0])\n",
    "    return all_samples[0], all_samples[1], all_samples[2]\n",
    "\n",
    "\n",
    "dataset_dict = {\n",
    "    \"ERP_ERP_ANA\": 0,\n",
    "    \"RS_RS_ALPHA\": 1,\n",
    "    \"ERP_ERP_BISC\": 2,\n",
    "    \"ERP_ERP_BBI\": 3,\n",
    "    \"ERP_ERP_BICF\": 4,\n",
    "    \"ERP_ERP_BICD\": 5,\n",
    "    \"RS_RS_SPIS\": 6,\n",
    "    \"MI_MI_HGD\": 7,\n",
    "    \"MI_MI_SCP\": 8,\n",
    "    \"ErrP_ErrP_MERP\": 9,\n",
    "    \"MI_MI_ULM\": 10,\n",
    "    \"MI_MI_VEP\": 11,\n",
    "    \"MI_MI_LR\": 12,\n",
    "    \"MI_BBCI_IV_Graz_b\": 13,\n",
    "    \"MI_MI_EB\": 14,\n",
    "    \"MI_BBCI_IV_Graz_a\": 15,\n",
    "    \"MI_MI_GVH_V\": 16,\n",
    "    \"MI_MI_GAL\": 17,\n",
    "    \"MI_MI_Two\": 18,\n",
    "    \"MI_MI_GVH_H\": 19,\n",
    "    \"MI_MI_II\": 20,\n",
    "    \"ErrP_ErrP_BCI\": 21,\n",
    "    \"MI_MI_GVH_G\": 22,\n",
    "    \"MI_MI_Limb\": 23,\n",
    "    \"MI_MI_SCI\": 24,\n",
    "    \"MI_BBCI_IV_Berlin\": 25,\n",
    "    \"MI_eegmmidb\": 26,\n",
    "    \"ERP_ERP_FHD\": 27,\n",
    "    \"RS_RS_EID\": 28,\n",
    "}\n",
    "\n",
    "\n",
    "def extract_dataset_name(file_path, dataset_dict):\n",
    "    for name in dataset_dict.keys():\n",
    "        if name in file_path:\n",
    "            return name\n",
    "    return \"Unknown\"\n",
    "\n",
    "def get_generic_channel_name(channel_name):\n",
    "    channel_name = channel_name.lower()\n",
    "    # Remove \"eeg \" prefix if present\n",
    "    if channel_name.startswith(\"eeg \"):\n",
    "        channel_name = channel_name[4:]\n",
    "    # Simplify names with a dash and check if it ends with \"-\"\n",
    "    if \"-\" in channel_name:\n",
    "        if channel_name.endswith(\"-\"):\n",
    "            return \"None\"\n",
    "        return channel_name.split(\"-\")[0]\n",
    "    return channel_name\n",
    "\n",
    "def load_edf_to_dataframe(file_path):\n",
    "    eeg_data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    channel_data_dict = {}\n",
    "\n",
    "    for channel in eeg_data.ch_names:\n",
    "        idx = eeg_data.ch_names.index(channel)\n",
    "        channel = get_generic_channel_name(channel)\n",
    "        data, times = eeg_data[idx, :]\n",
    "        channel_data_dict[channel] = data.flatten()\n",
    "\n",
    "    df = pd.DataFrame(channel_data_dict)\n",
    "    df['Time in Seconds'] = times.flatten()\n",
    "    return df\n",
    "\n",
    "def load_file_data(data_index, task_channels):\n",
    "    num_samples = 0\n",
    "    data = {}\n",
    "    outputs = {}\n",
    "    srs = {}\n",
    "    durs = {}\n",
    "    channels = {}\n",
    "    datasets = {}\n",
    "    failed_samples = []\n",
    "\n",
    "    for sample in tqdm(data_index, desc=\"Loading data\", position=0, leave=True):\n",
    "        try:\n",
    "            # Load the data of this sample\n",
    "            input_files = sample[\"input\"]\n",
    "\n",
    "            if load_mode == 2:\n",
    "                file = prefix_filepath + input_files[0] if \"/itet-stor\" not in input_files[0] else input_files[0]\n",
    "                df = load_edf_to_dataframe(file)\n",
    "                datasets[num_samples] = \"TUEG\"\n",
    "            else:\n",
    "                dataframes = [pd.read_pickle(filepath) for filepath in input_files]\n",
    "                df = pd.concat(dataframes, axis=0)\n",
    "                dataset_name = extract_dataset_name(file, dataset_dict)\n",
    "                datasets[num_samples] = dataset_name\n",
    "\n",
    "            # Crop the data to the desired length\n",
    "            start = int(sample[\"start\"])\n",
    "            length = int(sample[\"length\"]) if \"length\" in sample else int(sample[\"end\"])\n",
    "            df.loc[start : start + length, :] if load_mode==1 else df.iloc[start:length, :]\n",
    "            assert len(df) > 0, f\"Empty dataframe for sample: {sample}\"\n",
    "   \n",
    "            sr = int(\n",
    "                1 / float(float(df[time_col].iloc[1]) - float(df[time_col].iloc[0]))\n",
    "            )\n",
    "            if load_mode != 1:\n",
    "                outputs[num_samples] = (\n",
    "                    sample[\"output\"] if \"output\" in sample else sample[\"label\"]\n",
    "                )\n",
    "            else:\n",
    "                if task_name == \"EyeNetPosition\":\n",
    "                    outputs[num_samples] = list(sample[\"output\"].values())\n",
    "                else:\n",
    "                    outputs[num_samples] = list(sample[\"output\"].values())[0]\n",
    "            srs[num_samples] = sr\n",
    "            durs[num_samples] = len(df) / sr\n",
    "            channels[num_samples] = sorted(list(set(df.columns) & set(task_channels)), key=lambda x: list(task_channels).index(x))\n",
    "            df = df[channels[num_samples]].astype(float)\n",
    "            signals = torch.tensor(df.to_numpy(), dtype=torch.float32).T\n",
    "            data[num_samples] = signals\n",
    "            num_samples += 1\n",
    "            del df\n",
    "\n",
    "            if num_samples % 100 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process sample: {sample}. Error: {e}\", file=sys.stderr)\n",
    "            failed_samples.append(sample)\n",
    "\n",
    "    return data, outputs, srs, durs, channels, datasets\n",
    "\n",
    "\n",
    "if load_mode != 1:\n",
    "    print(json_path, file=sys.stderr)\n",
    "    train_index, test_index = load_index0(json_path)\n",
    "else:\n",
    "    train_index, val_index, test_index = load_index1(json_path)\n",
    "\n",
    "print(f\"Full train size: {len(train_index)}\", file=sys.stderr)\n",
    "print(f\"Full test size: {len(test_index)}\", file=sys.stderr)\n",
    "\n",
    "if load_mode != 1:\n",
    "    train_index = train_index[:100] + train_index[-100:]\n",
    "    test_index = test_index[:50] + test_index[-50:]\n",
    "else:\n",
    "    train_index = train_index\n",
    "    val_index = val_index\n",
    "    test_index = test_index\n",
    "\n",
    "if load_mode == 0 or load_mode == 2:\n",
    "    print(\"=\" * 10 + \"Load train data\" + \"=\" * 100)\n",
    "    train_data, train_outputs, train_sr, train_dur, train_channels, train_datasets = (\n",
    "        load_file_data(train_index, task_channels)\n",
    "    )\n",
    "    print(\"=\" * 10 + \"Load test data\" + \"=\" * 100)\n",
    "    test_data, test_outputs, test_sr, test_dur, test_channels, test_datasets = (\n",
    "        load_file_data(test_index, task_channels)\n",
    "    )\n",
    "elif load_mode == 1:\n",
    "    train_data, train_outputs, train_sr, train_dur, train_channels, train_datasets = (\n",
    "        load_file_data(train_index, task_channels)\n",
    "    )\n",
    "    val_data, val_outputs, val_sr, val_dur, val_channels, val_datasets = load_file_data(\n",
    "        val_index, task_channels\n",
    "    )\n",
    "    test_data, test_outputs, test_sr, test_dur, test_channels, test_datasets = (\n",
    "        load_file_data(test_index, task_channels)\n",
    "    )\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Label Encoder & Class Weights\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "if isinstance(list(train_outputs.values())[0], str):\n",
    "    all_outputs = list(set(list(train_outputs.values()) + list(test_outputs.values())))\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_outputs)\n",
    "\n",
    "    print(f\"Train classes: {set(train_outputs.values())}\", file=sys.stderr)\n",
    "    print(f\"Test classes: {set(test_outputs.values())}\", file=sys.stderr)\n",
    "\n",
    "    # Encode the train and test outputs\n",
    "    encoded_train_outputs = {\n",
    "        k: label_encoder.transform([v])[0] for k, v in train_outputs.items()\n",
    "    }\n",
    "    encoded_test_outputs = {\n",
    "        k: label_encoder.transform([v])[0] for k, v in test_outputs.items()\n",
    "    }\n",
    "\n",
    "    # Create the output counts map\n",
    "    train_output_counts = defaultdict(int)\n",
    "    for output in encoded_train_outputs.values():\n",
    "        train_output_counts[output] += 1\n",
    "\n",
    "    test_output_counts = defaultdict(int)\n",
    "    for output in encoded_test_outputs.values():\n",
    "        test_output_counts[output] += 1\n",
    "\n",
    "    full_output_counts = train_output_counts.copy()\n",
    "    for output, count in test_output_counts.items():\n",
    "        full_output_counts[output] += count\n",
    "\n",
    "    print(\"Full Output Counts:\", full_output_counts, file=sys.stderr)\n",
    "\n",
    "    # Calculate class weights\n",
    "    total_count = sum(full_output_counts.values())\n",
    "    class_weights = {\n",
    "        output: total_count / count for output, count in full_output_counts.items()\n",
    "    }\n",
    "\n",
    "    # Convert class weights to a tensor\n",
    "    weight_tensor = torch.tensor(\n",
    "        [class_weights[i] for i in range(len(class_weights))], dtype=torch.float\n",
    "    )\n",
    "else:\n",
    "    label_encoder = None\n",
    "    weight_tensor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db9d9b-e3e5-408a-a3f1-c0815e6120c6",
   "metadata": {},
   "source": [
    "# Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209adf01-7017-441f-89ca-26f3ed83bcec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70195951-8bb9-4c86-b150-093cee423457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 30 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.9 /itet-stor/maxihuber/net_scratch/conda_envs/faste ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                         | Type              | Params | Mode \n",
      "---------------------------------------------------------------------------\n",
      "0 | encoder                      | EncoderViTRoPE    | 23.1 M | train\n",
      "1 | finetune_time_transformer    | SimpleTransformer | 1.8 M  | train\n",
      "2 | finetune_channel_transformer | SimpleTransformer | 1.8 M  | train\n",
      "3 | head                         | Linear            | 770    | train\n",
      "4 | criterion                    | CrossEntropyLoss  | 0      | train\n",
      "---------------------------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "23.1 M    Non-trainable params\n",
      "26.7 M    Total params\n",
      "106.686   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: YC\n",
      "Task: Epilepsy (Classification)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce55ee95cd36448abc074a38866ecff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 1226, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 1226, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 602, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 602, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([20, 1202, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([20, 1202, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([20, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 20, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 674, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 674, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([20, 1202, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([20, 1202, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([20, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 20, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 2546, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 2546, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 1202, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 1202, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 1034, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 1034, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 602, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 602, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 37, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 37, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 1202, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 1202, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 602, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 602, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 1202, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 1202, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 2002, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 2002, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 1766, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 1766, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 1202, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 1202, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([20, 1202, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([20, 1202, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([20, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 20, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 1766, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 1766, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 156, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 156, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([20, 2002, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([20, 2002, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([20, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 20, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 1202, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 1202, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 1766, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 1766, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 1766, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 1766, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 642, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 642, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 2754, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 2754, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 602, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 602, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 602, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 602, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 30, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 30, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 1766, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 1766, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 602, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 602, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n",
      "[FT.forward, after self.encoder] x_emb.shape: torch.Size([22, 4002, 384])\n",
      "[FT.forward, before self.time_transformer] x_emb.shape: torch.Size([22, 4002, 384])\n",
      "[FT.forward, after time-token] x_emb.shape: torch.Size([22, 384])\n",
      "[FT.forward, before channel-token] x_emb.shape: torch.Size([1, 22, 384])\n",
      "[FT.forward, after channel-token] x_emb.shape: torch.Size([384])\n",
      "collate_fn\n",
      "[FT.forward] win_shifts: dict_keys([1])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 29.55 GiB. GPU 0 has a total capacty of 47.54 GiB of which 11.42 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 32.40 GiB is allocated by PyTorch, and 3.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 481\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 481\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfine_tuning_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model\u001b[38;5;241m=\u001b[39mfine_tuning_model, dataloaders\u001b[38;5;241m=\u001b[39mtest_loader)\n\u001b[1;32m    484\u001b[0m final_checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/itet-stor/maxihuber/net_scratch/finetune_ckpts/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final_model.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         closure()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/core/module.py:1296\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1267\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1271\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \n\u001b[1;32m   1295\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/core/optimizer.py:152\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 143\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    146\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     97\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:318\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 334\u001b[0m, in \u001b[0;36mFineTuningModel.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m    333\u001b[0m     x, y, dataset \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m--> 334\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39my_hat, target\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 291\u001b[0m, in \u001b[0;36mFineTuningModel.forward\u001b[0;34m(self, full_x)\u001b[0m\n\u001b[1;32m    286\u001b[0m B, C, H, W \u001b[38;5;241m=\u001b[39m spgs\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# TODO: split into less rows if necessary because of CUDA error\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m#nr_tokens = B * C * H * W\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m#if nr_tokens > max_nr_tokens:\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m#    pass\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m x_emb, _, _, nr_meta_patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# TODO: \u001b[39;00m\n\u001b[1;32m    300\u001b[0m x_embeds[win_size] \u001b[38;5;241m=\u001b[39m x_emb\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/eeg-foundation/src/models/mae_rope_encoder.py:224\u001b[0m, in \u001b[0;36mEncoderViTRoPE.forward\u001b[0;34m(self, x, means, stds, channels, win_size, mask_ratio)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# print(\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m#     \"[forward_encoder] freqs_cis.shape:\",\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m#     freqs_cis.shape,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m \n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Encoder: apply the encoder blocks\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_blocks:\n\u001b[0;32m--> 224\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_meta_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnr_meta_patches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# print(\"[forward_encoder] after rope blocks:\", x.shape, \"(B, N, D)\")\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Encoder: normalize the output\u001b[39;00m\n\u001b[1;32m    228\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_norm(x)\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/eeg-foundation/src/models/components/vit_rope.py:210\u001b[0m, in \u001b[0;36mFlexible_RoPE_Layer_scale_init_Block.forward\u001b[0;34m(self, x, freqs_cis, nr_meta_tokens)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, freqs_cis, nr_meta_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    208\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma_1\n\u001b[0;32m--> 210\u001b[0m         \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_meta_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnr_meta_tokens\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     )\n\u001b[1;32m    214\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma_2 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)))\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/eeg-foundation/src/models/components/vit_rope.py:171\u001b[0m, in \u001b[0;36mFlexibleRoPEAttention.forward\u001b[0;34m(self, x, freqs_cis, nr_meta_tokens)\u001b[0m\n\u001b[1;32m    167\u001b[0m q[:, :, nr_meta_tokens:], k[:, :, nr_meta_tokens:] \u001b[38;5;241m=\u001b[39m apply_rotary_emb(\n\u001b[1;32m    168\u001b[0m     q[:, :, nr_meta_tokens:], k[:, :, nr_meta_tokens:], freqs_cis\u001b[38;5;241m=\u001b[39mfreqs_cis\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    170\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(attn)\n\u001b[1;32m    174\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, N, C)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 29.55 GiB. GPU 0 has a total capacty of 47.54 GiB of which 11.42 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 32.40 GiB is allocated by PyTorch, and 3.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "L.seed_everything(42)\n",
    "\n",
    "ckpt_path = '/itet-stor/maxihuber/net_scratch/checkpoints/980473/epoch=7-step=239317-val_loss=130.45-lr.ckpt'\n",
    "ckpt_path = '/itet-stor/maxihuber/net_scratch/checkpoints/977598/epoch=0-step=32807-val_loss=133.55.ckpt'\n",
    "\n",
    "#########################################################################################################\n",
    "class FinetuneDataset(Dataset):\n",
    "    def __init__(self, data, outputs, srs, durs, channels, datasets, task_type, label_encoder=None):\n",
    "        self.data = data\n",
    "        self.outputs = outputs\n",
    "        self.srs = srs\n",
    "        self.durs = durs\n",
    "        self.channels = channels\n",
    "        self.datasets = datasets\n",
    "        self.task_type = task_type\n",
    "        self.label_encoder = label_encoder\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        signals = self.data[idx]\n",
    "        output = self.outputs[idx]\n",
    "        sr = self.srs[idx]\n",
    "        dur = self.durs[idx]\n",
    "        channels = self.channels[idx]\n",
    "        dataset = self.datasets[idx]\n",
    "\n",
    "        if self.task_type == \"Classification\" and self.label_encoder is not None:\n",
    "            output = self.label_encoder.transform([output])[0]  # Encode the output label\n",
    "            output_tensor = torch.tensor(output, dtype=torch.long)\n",
    "        else:\n",
    "            if task_name == \"EyeNetPosition\":\n",
    "                output_tensor = torch.tensor(output, dtype=torch.float32)\n",
    "            else:\n",
    "                output_tensor = torch.tensor([output], dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            \"signals\": signals,\n",
    "            \"output\": output_tensor,\n",
    "            \"sr\": sr,\n",
    "            \"dur\": dur,\n",
    "            \"channels\": channels,\n",
    "            \"dataset\": dataset\n",
    "        }\n",
    "\n",
    "if load_mode != 1:\n",
    "    full_train_dataset = FinetuneDataset(train_data, train_outputs, train_sr, train_dur, train_channels, train_datasets, task_type=task_type, label_encoder=label_encoder)\n",
    "    test_dataset = FinetuneDataset(test_data, test_outputs, test_sr, test_dur, test_channels, test_datasets, task_type=task_type, label_encoder=label_encoder)\n",
    "    # Define the split ratio\n",
    "    train_ratio = 0.85\n",
    "    val_ratio = 0.15\n",
    "    # Calculate lengths for train and validation sets\n",
    "    total_size = len(full_train_dataset)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    val_size = total_size - train_size\n",
    "    # Split the dataset\n",
    "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "elif load_mode == 1:\n",
    "    train_dataset = FinetuneDataset(train_data, train_outputs, train_sr, train_dur, train_channels, train_datasets, task_type=task_type, label_encoder=label_encoder)\n",
    "    val_dataset = FinetuneDataset(val_data, val_outputs, val_sr, val_dur, val_channels, val_datasets, task_type=task_type, label_encoder=label_encoder)\n",
    "    test_dataset = FinetuneDataset(test_data, test_outputs, test_sr, test_dur, test_channels, test_datasets, task_type=task_type, label_encoder=label_encoder)\n",
    "else:\n",
    "    pass\n",
    "    \n",
    "#########################################################################################################\n",
    "# DataLoaders\n",
    "self_win_shifts = [.25, .5, 1, 2, 4, 8]\n",
    "self_patch_size = 16\n",
    "self_win_shift_factor = .25\n",
    "self_max_win_shift = self_win_shifts[-1]\n",
    "self_max_y_datapoints = 4_000\n",
    "max_nr_patches = 8_500\n",
    "\n",
    "def get_nr_y_patches(win_size, sr):\n",
    "    return int((sr / 2 * win_size + 1) / self_patch_size)\n",
    "\n",
    "def get_nr_x_patches(win_size, dur):\n",
    "    win_shift = win_size * self_win_shift_factor\n",
    "    x_datapoints_per_second = 1 / win_shift\n",
    "    x_datapoints = dur * x_datapoints_per_second + 1\n",
    "    return int(x_datapoints // self_patch_size)\n",
    "\n",
    "channel_name_map_path = '/home/maxihuber/eeg-foundation/src/data/components/channels_to_id.json'\n",
    "with open(channel_name_map_path, \"r\") as file:\n",
    "    self_channel_name_map = json.load(file)\n",
    "\n",
    "def self_get_generic_channel_name(channel_name):\n",
    "    channel_name = channel_name.lower()\n",
    "    # Remove \"eeg \" prefix if present\n",
    "    if channel_name.startswith(\"eeg \"):\n",
    "        channel_name = channel_name[4:]\n",
    "    # Simplify names with a dash and check if it ends with \"-\"\n",
    "    if \"-\" in channel_name:\n",
    "        if channel_name.endswith(\"-\"):\n",
    "            return \"None\"\n",
    "        return channel_name.split(\"-\")[0]\n",
    "    return channel_name\n",
    "\n",
    "def self_encode_mean(mean, win_size):\n",
    "    y_datapoints = mean.shape[0]\n",
    "    encoded_mean = torch.zeros(self_max_y_datapoints)\n",
    "    step_size = int(self_max_win_shift // win_size)\n",
    "    end_idx = step_size * y_datapoints\n",
    "    indices = torch.arange(0, end_idx, step_size)\n",
    "    encoded_mean[indices] = mean.squeeze_().float()\n",
    "    encoded_mean.unsqueeze_(1)\n",
    "    return encoded_mean\n",
    "\n",
    "#########################################################################################################\n",
    "# collate_fn\n",
    "# make batches as the pre-trained network expects (channel tokens, means, standard deviation etc.)\n",
    "def sample_collate_fn(batch):\n",
    "\n",
    "    signals, output, sr, dur, channels, dataset = batch[0][\"signals\"], batch[0][\"output\"], batch[0][\"sr\"], batch[0][\"dur\"], batch[0][\"channels\"], batch[0][\"dataset\"]\n",
    "\n",
    "    if dur > 1_000:\n",
    "        dur = 1_000\n",
    "        signals = signals[:, :1_000*sr]\n",
    "    \n",
    "    valid_win_shifts = \"\"\"\n",
    "    # TODO: compute spectrograms for each win_size\n",
    "    # gives a new dimension (S) in batch\n",
    "    # need another extra transformer after the encoder\n",
    "    # (B, 1, H, W) -> (S*B, 1, H, W)\n",
    "    valid_win_shifts = [\n",
    "        win_shift\n",
    "        for win_shift in self_win_shifts\n",
    "        if get_nr_y_patches(win_shift, sr) >= 1\n",
    "        and get_nr_x_patches(win_shift, dur) >= 1\n",
    "        and get_nr_y_patches(win_shift, sr) * get_nr_x_patches(win_shift, dur) < max_nr_patches\n",
    "    ]\n",
    "\n",
    "    assert valid_win_shifts != [], \"no valid win_shift found\"\n",
    "    \"\"\"\n",
    "    valid_win_shifts = [1]\n",
    "\n",
    "    # list holding assembled tensors for varying window shifts\n",
    "    full_batch = {}   \n",
    "\n",
    "    for win_size in valid_win_shifts:\n",
    "        \n",
    "        fft = torchaudio.transforms.Spectrogram(\n",
    "            n_fft=int(sr * win_size),\n",
    "            win_length=int(sr * win_size),\n",
    "            hop_length=int(sr * win_size * self_win_shift_factor),\n",
    "            normalized=True,\n",
    "        )\n",
    "    \n",
    "        spg_list = []\n",
    "        chn_list = []\n",
    "        mean_list = []\n",
    "        std_list = []\n",
    "    \n",
    "        for signal, channel in zip(signals, channels):\n",
    "            \n",
    "            # Channel information\n",
    "            channel_name = self_get_generic_channel_name(channel)\n",
    "            channel = self_channel_name_map[channel_name] if channel_name in self_channel_name_map else self_channel_name_map[\"None\"]\n",
    "    \n",
    "            # Spectrogram Computation & Cropping\n",
    "            spg = fft(signal)\n",
    "            spg = spg**2\n",
    "            spg = crop_spg(spg, self_patch_size)\n",
    "            \n",
    "            H_new, W_new = spg.shape[0], spg.shape[1]\n",
    "            h_new, w_new = H_new // self_patch_size, W_new // self_patch_size\n",
    "    \n",
    "            # Prepare channel information (per-patch)\n",
    "            channel = torch.full((h_new, w_new), channel, dtype=torch.float16)\n",
    "            \n",
    "            spg, mean, std = normalize_spg(spg)\n",
    "            mean = self_encode_mean(mean, win_size)\n",
    "            std = self_encode_mean(std, win_size)\n",
    "            \n",
    "            spg_list.append(spg)\n",
    "            chn_list.append(channel)\n",
    "            mean_list.append(mean)\n",
    "            std_list.append(std)\n",
    "        \n",
    "        win_batch = torch.stack(spg_list)\n",
    "        win_channels = torch.stack(chn_list)\n",
    "        win_means = torch.stack(mean_list)\n",
    "        win_stds = torch.stack(std_list)\n",
    "        \n",
    "        win_batch.unsqueeze_(1)\n",
    "        win_channels = win_channels.flatten(1)\n",
    "        win_means = win_means.transpose(1, 2)\n",
    "        win_stds = win_stds.transpose(1, 2)\n",
    "        \n",
    "        full_batch[win_size] = {\n",
    "            \"batch\": win_batch,\n",
    "            \"channels\": win_channels,\n",
    "            \"means\": win_means,\n",
    "            \"stds\": win_stds\n",
    "        }\n",
    "        #print(f\"[collate_fn] win_size={win_size}: {win_batch.shape}\")\n",
    "        \n",
    "    # == Finished iterating over all possible window shifts\n",
    "    print(\"collate_fn\")\n",
    "    return full_batch, output, dataset\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, collate_fn=sample_collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=sample_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=sample_collate_fn)\n",
    "\n",
    "print(len(train_loader), len(val_loader), len(test_loader))\n",
    "\n",
    "#########################################################################################################\n",
    "# Model\n",
    "# == Metrics ==\n",
    "def rmse(y_true, y_pred):\n",
    "    return torch.sqrt(torch.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    return balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "class SingleTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(SingleTransformerEncoderLayer, self).__init__()\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model, nhead)\n",
    "\n",
    "    def forward(self, src):\n",
    "        return self.encoder_layer(src)\n",
    "\n",
    "def mean_aggregation(tokens):\n",
    "    return torch.mean(torch.stack(tokens), dim=0)\n",
    "\n",
    "class FineTuningModel(L.LightningModule):\n",
    "    def __init__(self, encoder, frozen_encoder, out_dim, task_name, task_type, learning_rate, mask_ratio):\n",
    "        super(FineTuningModel, self).__init__()\n",
    "\n",
    "        self.task_name = task_name\n",
    "        self.task_type = task_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        # Pretrained network\n",
    "        self.encoder = encoder       \n",
    "        if frozen_encoder:\n",
    "            self.freeze_encoder()\n",
    "\n",
    "        # Finetuning network\n",
    "        self.finetune_time_transformer = SimpleTransformer(\n",
    "            embed_size=384,\n",
    "            max_len=8_5000\n",
    "        )\n",
    "        \n",
    "        self.finetune_channel_transformer = SimpleTransformer(\n",
    "            embed_size=384,\n",
    "            max_len=200,\n",
    "        )\n",
    "        \n",
    "        # Modular aggregation method on channel tokens\n",
    "        self.win_shift_aggregation = mean_aggregation\n",
    "        \n",
    "        if task_type == \"Regression\":\n",
    "            self.head = nn.Linear(encoder.encoder_embed_dim, 1)\n",
    "            self.criterion = nn.MSELoss()\n",
    "        else:\n",
    "            self.head = nn.Linear(encoder.encoder_embed_dim, 1)\n",
    "            self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.train_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, full_x):\n",
    "        x_embeds = {}\n",
    "        H_W = {}\n",
    "\n",
    "        print(f\"[FT.forward] win_shifts: {full_x.keys()}\")\n",
    "        \n",
    "        for win_size, x_win in full_x.items():\n",
    "            spgs = x_win[\"batch\"]\n",
    "            channels = x_win[\"channels\"]\n",
    "            means = x_win[\"means\"]\n",
    "            stds = x_win[\"stds\"]\n",
    "            B, C, H, W = spgs.shape\n",
    "            x_emb, _, _, nr_meta_patches = self.encoder(\n",
    "                x=spgs,\n",
    "                means=means,\n",
    "                stds=stds,\n",
    "                channels=channels,\n",
    "                win_size=win_size,\n",
    "                mask_ratio=self.mask_ratio,\n",
    "            )\n",
    "            x_embeds[win_size] = x_emb\n",
    "            H_W[win_size] = (H, W)\n",
    "            print(f\"[FT.forward, after self.encoder] x_emb.shape: {x_emb.shape}\")\n",
    "\n",
    "        # Pass through time-transformer\n",
    "        for win_size, x_emb in x_embeds.items():\n",
    "            print(f\"[FT.forward, before self.time_transformer] x_emb.shape: {x_emb.shape}\")\n",
    "            x_emb = self.finetune_time_transformer(x_emb)\n",
    "            x_emb = x_emb[:, 0]\n",
    "            print(f\"[FT.forward, after time-token] x_emb.shape: {x_emb.shape}\")\n",
    "            x_embeds[win_size] = x_emb\n",
    "\n",
    "        # Pass through channel-transformer\n",
    "        tokens = []\n",
    "        for win_size, x_emb in x_embeds.items():\n",
    "            x_emb = x_emb.unsqueeze(0)\n",
    "            print(f\"[FT.forward, before channel-token] x_emb.shape: {x_emb.shape}\")\n",
    "            x_emb = self.finetune_channel_transformer(x_emb)\n",
    "            x_emb = x_emb[0, 0]\n",
    "            print(f\"[FT.forward, after channel-token] x_emb.shape: {x_emb.shape}\")\n",
    "            tokens.append(x_emb)\n",
    "\n",
    "        # Average over all window shifts\n",
    "        smart_token = self.win_shift_aggregation(tokens)\n",
    "\n",
    "        # Pass through head\n",
    "        y_hat = self.head(smart_token).squeeze()\n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, dataset = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(input=y_hat, target=y.float())\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "\n",
    "        if self.task_type == \"Classification\":\n",
    "            y_pred = (torch.sigmoid(y_hat) >= 0.5).float()\n",
    "            self.train_step_outputs.append((y.cpu(), y_pred.cpu(), dataset))\n",
    "        elif self.task_type == \"Regression\":\n",
    "            self.train_step_outputs.append((y.cpu(), y_hat.cpu(), dataset))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.compute_metrics(self.train_step_outputs, 'train')\n",
    "        self.train_step_outputs.clear()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, dataset = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(input=y_hat, target=y.float())\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "        if self.task_type == \"Classification\":\n",
    "            y_pred = (torch.sigmoid(y_hat) >= 0.5).float()\n",
    "            self.validation_step_outputs.append((y.cpu(), y_pred.cpu(), dataset))\n",
    "        elif self.task_type == \"Regression\":\n",
    "            self.validation_step_outputs.append((y.cpu(), y_hat.cpu(), dataset))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.compute_metrics(self.validation_step_outputs, 'val')\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, dataset = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(input=y_hat, target=y.float())\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "\n",
    "        if self.task_type == \"Classification\":\n",
    "            y_pred = (torch.sigmoid(y_hat) >= 0.5).float()\n",
    "            self.test_step_outputs.append((y.cpu(), y_pred.cpu(), dataset))\n",
    "        elif self.task_type == \"Regression\":\n",
    "            self.test_step_outputs.append((y.cpu(), y_hat.cpu(), dataset))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.compute_metrics(self.test_step_outputs, 'test')\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def compute_metrics(self, outputs, stage):\n",
    "        y_true_all = defaultdict(list)\n",
    "        y_pred_all = defaultdict(list)\n",
    "        \n",
    "        for y_true, y_pred, dataset in outputs:\n",
    "            y_true_all[dataset].append(y_true)\n",
    "            y_pred_all[dataset].append(y_pred)\n",
    "\n",
    "        overall_y_true = []\n",
    "        overall_y_pred = []\n",
    "\n",
    "        for dataset in y_true_all.keys():\n",
    "            y_true_cat = torch.stack(y_true_all[dataset])\n",
    "            y_pred_cat = torch.stack(y_pred_all[dataset])\n",
    "\n",
    "            overall_y_true.append(y_true_cat)\n",
    "            overall_y_pred.append(y_pred_cat)\n",
    "\n",
    "            if self.task_type == \"Classification\":\n",
    "                balanced_acc = balanced_accuracy_score(y_true_cat, y_pred_cat)\n",
    "                self.log(f'{stage}_balanced_accuracy_{dataset}', balanced_acc, prog_bar=True)\n",
    "            elif self.task_type == \"Regression\":\n",
    "                rmse_value = rmse(y_true_cat, y_pred_cat)\n",
    "                self.log(f'{stage}_rmse_{dataset}', rmse_value, prog_bar=True)\n",
    "\n",
    "        # Compute overall metrics\n",
    "        overall_y_true = torch.cat(overall_y_true, dim=0)\n",
    "        overall_y_pred = torch.cat(overall_y_pred, dim=0)\n",
    "\n",
    "        if self.task_type == \"Classification\":\n",
    "            balanced_acc = balanced_accuracy_score(overall_y_true, overall_y_pred)\n",
    "            self.log(f'{stage}_balanced_accuracy', balanced_acc, prog_bar=True)\n",
    "        elif self.task_type == \"Regression\":\n",
    "            rmse_value = rmse(overall_y_true, overall_y_pred)\n",
    "            self.log(f'{stage}_rmse', rmse_value, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.head.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        if self.trainer.current_epoch == 1:\n",
    "            self.unfreeze_encoder()\n",
    "            print(f\"Unfroze encoder at epoch {self.trainer.current_epoch}\")\n",
    "        \n",
    "    def freeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "#########################################################################################################\n",
    "# Load the checkpoint\n",
    "chkpt_path = ckpt_path\n",
    "checkpoint = torch.load(chkpt_path, map_location=torch.device('cpu'))\n",
    "state_dict = checkpoint['state_dict']\n",
    "state_dict = {k.replace(\"net.encoder.\", \"\"): v for k, v in state_dict.items() if \"net.encoder.\" in k}\n",
    "\n",
    "# Initialize the encoder and load the state dict\n",
    "encoder = EncoderViTRoPE(channel_name_map_path)\n",
    "encoder.load_state_dict(state_dict)\n",
    "\n",
    "# Instantiate the fine-tuning model\n",
    "fine_tuning_model = FineTuningModel(encoder=encoder,\n",
    "                                    frozen_encoder=True,\n",
    "                                    out_dim=out_dim,\n",
    "                                    task_name=task_name,\n",
    "                                    task_type=task_type,\n",
    "                                    learning_rate=0.01,\n",
    "                                    mask_ratio=0)\n",
    "\n",
    "#########################################################################################################\n",
    "# Define the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"/itet-stor/maxihuber/deepeye_storage/finetune_ckpts/{task_name}\",\n",
    "    filename=\"{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    log_every_n_steps=1,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "print(f\"Class: {class_name}\")\n",
    "print(f\"Task: {task_name} ({task_type})\")\n",
    "trainer.fit(fine_tuning_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "trainer.test(model=fine_tuning_model, dataloaders=test_loader)\n",
    "final_checkpoint_path = f\"/itet-stor/maxihuber/net_scratch/finetune_ckpts/{task_name}/final_model.ckpt\"\n",
    "trainer.save_checkpoint(final_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5260530a-822e-466a-83ea-6ee4a27bc501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'luc', 'p4', 'c3', 'sp2', '21', 'pz', 'fp2', 't6', 'fz', 't2', '25', '27', '28', 'rlc', 'loc', 'f4', 'f8', '30', '31', 'roc', '29', 't4', 't3', 'p3', 'a2', 'oz', 'ekg', 'a1', 't5', 't1', 'pg1', 'c4p', 'cz', 'c4', 'fp1', 'o1', 'o2', '24', 'f3', '22', '23', '26', 'f7', '32', 'c3p', 'pg2', '20', 'ekg1', 'sp1'}\n"
     ]
    }
   ],
   "source": [
    "with open('/home/maxihuber/eeg-foundation/runs/eps_trainchns.json', 'r') as file:\n",
    "    train_chns = json.load(file)\n",
    "\n",
    "with open('/home/maxihuber/eeg-foundation/runs/eps_testchns.json', 'r') as file:\n",
    "    test_chns = json.load(file)\n",
    "\n",
    "all_chns = train_chns + test_chns\n",
    "\n",
    "chns = set()\n",
    "for f_chns in all_chns:\n",
    "    chns.update(f_chns)\n",
    "\n",
    "print(chns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d7502a-6b61-4828-bf1c-e7b8f5cddc66",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f65a6-9ecf-4f1a-89cd-7ac4d06c8757",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preparation & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a4ad85-e544-4be1-8061-a1dd64d985c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "L.seed_everything(42)\n",
    "sys.path.append(\"/home/maxihuber/eeg-foundation/src/models/components/Baselines\")\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data, outputs, datasets, task_type, label_encoder=None):\n",
    "        self.data = data\n",
    "        self.outputs = outputs\n",
    "        self.datasets = datasets\n",
    "        self.task_type = task_type\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signals = self.data[idx]\n",
    "        output = self.outputs[idx]\n",
    "        dataset = self.datasets[idx]\n",
    "\n",
    "        if self.task_type == \"Classification\" and self.label_encoder is not None:\n",
    "            output = self.label_encoder.transform([output])[\n",
    "                0\n",
    "            ]  # Encode the output label\n",
    "            output_tensor = torch.tensor(output, dtype=torch.long)\n",
    "        else:\n",
    "            output_tensor = torch.tensor([output], dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"signals\": signals,\n",
    "            \"output\": output_tensor,\n",
    "            \"dataset\": dataset,\n",
    "        }\n",
    "\n",
    "\n",
    "durs = [df.shape[1] for idx, df in train_data.items()] + [\n",
    "    df.shape[1] for idx, df in test_data.items()\n",
    "]\n",
    "n_chns = [df.shape[0] for idx, df in train_data.items()] + [\n",
    "    df.shape[0] for idx, df in test_data.items()\n",
    "]\n",
    "dur_90 = int(np.percentile(durs, 90))\n",
    "chn_90 = 128  # int(np.percentile(n_chns, 90))\n",
    "\n",
    "\n",
    "def pad_tensor(tensor, target_height, target_width):\n",
    "    current_height, current_width = tensor.shape\n",
    "\n",
    "    # Pad height if necessary\n",
    "    if current_height < target_height:\n",
    "        padding_height = target_height - current_height\n",
    "        padding = torch.zeros((padding_height, current_width), dtype=tensor.dtype)\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "    else:\n",
    "        tensor = tensor[:target_height, :]\n",
    "\n",
    "    # Pad width if necessary\n",
    "    if current_width < target_width:\n",
    "        padding_width = target_width - current_width\n",
    "        padding = torch.zeros((tensor.shape[0], padding_width), dtype=tensor.dtype)\n",
    "        tensor = torch.cat((tensor, padding), dim=1)\n",
    "    else:\n",
    "        tensor = tensor[:, :target_width]\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "train_data_pad = {\n",
    "    k: pad_tensor(signals, chn_90, dur_90) for k, signals in train_data.items()\n",
    "}\n",
    "test_data_pad = {\n",
    "    k: pad_tensor(signals, chn_90, dur_90) for k, signals in test_data.items()\n",
    "}\n",
    "\n",
    "full_train_dataset = SimpleDataset(\n",
    "    train_data_pad,\n",
    "    train_outputs,\n",
    "    train_datasets,\n",
    "    task_type=task_type,\n",
    "    label_encoder=label_encoder,\n",
    ")\n",
    "test_dataset = SimpleDataset(\n",
    "    test_data_pad,\n",
    "    test_outputs,\n",
    "    train_datasets,\n",
    "    task_type=task_type,\n",
    "    label_encoder=label_encoder,\n",
    ")\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio, val_ratio = 0.85, 0.15\n",
    "\n",
    "# Calculate lengths for train and validation sets\n",
    "total_size = len(full_train_dataset)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387a9b7-5b7a-45ad-9f9e-4136aa5f3541",
   "metadata": {},
   "source": [
    "## XDawn + LDA ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ea15d2-0db4-448e-b1dc-90e0206b4184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pad/Truncate signals:   0%|                                                                                                                                              | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_1751258/1339767812.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data[idx] = torch.tensor(signal_padded.clone().detach(), dtype=torch.float32)\n",
      "Pad/Truncate signals: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:17<00:00, 11.69it/s]\n",
      "Pad/Truncate signals: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 11.02it/s]\n",
      "Creating epochs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 238042.22it/s]\n",
      "Creating epochs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1253.32it/s]\n",
      "Balanced Accuracy: 0.51\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\n",
    "    f\"/itet-stor/maxihuber/net_scratch/finetune_ckpts/{task_name}\", exist_ok=True\n",
    ")\n",
    "\n",
    "# Function to resample signals\n",
    "def resample_signals(data, srs, target_sfreq):\n",
    "    resampled_data = {}\n",
    "    for idx, signal in tqdm(data.items(), desc=\"Resampling signals\"):\n",
    "        signal_numpy = signal.numpy().astype(np.float64)  # Convert to float64\n",
    "        signal_resampled = mne.filter.resample(signal_numpy, up=target_sfreq / srs[idx])\n",
    "        resampled_data[idx] = torch.tensor(signal_resampled, dtype=torch.float32)\n",
    "    return resampled_data\n",
    "\n",
    "\n",
    "# Function to pad or truncate signals to a common length\n",
    "def pad_or_truncate_signals(data, common_length):\n",
    "    for idx, signal in tqdm(data.items(), desc=\"Pad/Truncate signals\"):\n",
    "        signal_length = signal.shape[1]\n",
    "        if signal_length < common_length:\n",
    "            pad_width = common_length - signal_length\n",
    "            signal_padded = np.pad(signal, ((0, 0), (0, pad_width)), mode=\"constant\")\n",
    "        else:\n",
    "            signal_padded = signal[:, :common_length]\n",
    "        data[idx] = torch.tensor(signal_padded.clone().detach(), dtype=torch.float32)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Function to create MNE Epochs object from data\n",
    "def create_epochs(data, outputs, channels, sfreq=1000, is_classification=True):\n",
    "    events = []\n",
    "    event_id = {}\n",
    "    epochs_data = []\n",
    "    for idx, signal in tqdm(data.items(), desc=\"Creating epochs\"):\n",
    "        epochs_data.append(signal.numpy())\n",
    "        if is_classification:\n",
    "            if outputs[idx] not in event_id:\n",
    "                event_id[outputs[idx]] = len(event_id) + 1\n",
    "            events.append([idx, 0, event_id[outputs[idx]]])\n",
    "        else:\n",
    "            events.append([idx, 0, 1])  # Dummy event_id for regression\n",
    "    events = np.array(events, dtype=int)\n",
    "    info = mne.create_info(\n",
    "        ch_names=[f\"EEG_{i}\" for i in range(chn_90)], sfreq=sfreq, ch_types=\"eeg\"\n",
    "    )\n",
    "    epochs = mne.EpochsArray(\n",
    "        np.array(epochs_data),\n",
    "        info,\n",
    "        events=events,\n",
    "        event_id=event_id if is_classification else None,\n",
    "    )\n",
    "    return epochs\n",
    "\n",
    "\n",
    "# Determine the target sampling frequency (e.g., the highest or mean sampling rate)\n",
    "target_sfreq = int(max(train_sr.values()))  # or use statistics.mean(train_sr.values())\n",
    "\n",
    "# Resample train and test data\n",
    "# train_data_resampled = resample_signals(train_data_pad, train_sr, target_sfreq)\n",
    "# test_data_resampled = resample_signals(test_data_pad, test_sr, target_sfreq)\n",
    "train_data_resampled = train_data_pad\n",
    "test_data_resampled = test_data_pad\n",
    "\n",
    "# Determine the common length for all signals\n",
    "common_length = min(\n",
    "    min(signal.shape[1] for signal in train_data_resampled.values()),\n",
    "    min(signal.shape[1] for signal in test_data_resampled.values()),\n",
    ")\n",
    "\n",
    "# Pad or truncate train and test data to the common length\n",
    "train_data_padded = pad_or_truncate_signals(train_data_resampled, common_length)\n",
    "test_data_padded = pad_or_truncate_signals(test_data_resampled, common_length)\n",
    "\n",
    "# Convert train and test data to MNE Epochs\n",
    "is_classification = isinstance(list(train_outputs.values())[0], str)\n",
    "epochs_train = create_epochs(\n",
    "    train_data_padded,\n",
    "    train_outputs,\n",
    "    list(train_channels.values()),\n",
    "    target_sfreq,\n",
    "    is_classification,\n",
    ")\n",
    "epochs_test = create_epochs(\n",
    "    test_data_padded,\n",
    "    test_outputs,\n",
    "    list(test_channels.values()),\n",
    "    target_sfreq,\n",
    "    is_classification,\n",
    ")\n",
    "\n",
    "print(\"Start fitting Xdawn...\", file=sys.stderr)\n",
    "\n",
    "# Create an xDAWN instance and fit it to the training data\n",
    "xdawn = Xdawn(n_components=2, correct_overlap=False, reg=0.1)  # Adding regularization\n",
    "xdawn.fit(epochs_train)\n",
    "\n",
    "# Transform the data using xDAWN\n",
    "X_train_xdawn = xdawn.transform(epochs_train)\n",
    "X_test_xdawn = xdawn.transform(epochs_test)\n",
    "\n",
    "# Save xDAWN model parameters\n",
    "with open(\n",
    "    f\"/itet-stor/maxihuber/net_scratch/finetune_ckpts/{task_name}/xdawn_model.pkl\", \"wb\"\n",
    ") as f:\n",
    "    pickle.dump(xdawn, f)\n",
    "\n",
    "# Flatten the transformed data for LDA input\n",
    "n_epochs_train, n_components, n_times = X_train_xdawn.shape\n",
    "X_train_xdawn = X_train_xdawn.reshape(n_epochs_train, n_components * n_times)\n",
    "n_epochs_test, n_components, n_times = X_test_xdawn.shape\n",
    "X_test_xdawn = X_test_xdawn.reshape(n_epochs_test, n_components * n_times)\n",
    "\n",
    "if is_classification:\n",
    "    # Encode labels if they are strings (for classification tasks)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(list(train_outputs.values()))\n",
    "    y_test = label_encoder.transform(list(test_outputs.values()))\n",
    "\n",
    "    # Create an LDA instance and fit it to the transformed training data\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train_xdawn, y_train)\n",
    "\n",
    "    # Save LDA model parameters\n",
    "    with open(\n",
    "        f\"/itet-stor/maxihuber/net_scratch/finetune_ckpts/{task_name}/lda_model.pkl\",\n",
    "        \"wb\",\n",
    "    ) as f:\n",
    "        pickle.dump(lda, f)\n",
    "\n",
    "    # Predict the labels of the test set\n",
    "    y_pred = lda.predict(X_test_xdawn)\n",
    "\n",
    "    # Calculate metrics\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    print(f\"Balanced Accuracy: {balanced_acc}\", file=sys.stderr)\n",
    "else:\n",
    "    # For regression tasks\n",
    "    y_train = np.array(list(train_outputs.values()))\n",
    "    y_test = np.array(list(test_outputs.values()))\n",
    "\n",
    "    # Create a linear regression model and fit it to the transformed training data\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_xdawn, y_train)\n",
    "\n",
    "    # Save Linear Regression model parameters\n",
    "    with open(\n",
    "        f\"/itet-stor/maxihuber/net_scratch/finetune_ckpts/{task_name}/linear_regression_model.pkl\",\n",
    "        \"wb\",\n",
    "    ) as f:\n",
    "        pickle.dump(lr, f)\n",
    "\n",
    "    # Predict the values of the test set\n",
    "    y_pred = lr.predict(X_test_xdawn)\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmse_value = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"RMSE: {rmse_value}\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778e0ab-d9b2-444e-8f8a-ebd4b2ff8ceb",
   "metadata": {},
   "source": [
    "## Classifiers from Sklearn  ==========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e10e692-f09f-499a-bcef-a74abba07cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors\n",
      "Nearest Neighbors: 0.51\n",
      "Linear SVM\n",
      "Linear SVM: 0.59\n",
      "RBF SVM\n",
      "RBF SVM: 0.5\n",
      "Decision Tree\n",
      "Decision Tree: 0.6\n",
      "Random Forest\n",
      "Random Forest: 0.56\n",
      "AdaBoost\n"
     ]
    }
   ],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "# https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    # \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    # \"Neural Net (MLP)\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025, random_state=42),\n",
    "    SVC(gamma=2, C=1, random_state=42),\n",
    "    # GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n",
    "    DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    RandomForestClassifier(\n",
    "        max_depth=5, n_estimators=10, max_features=1, random_state=42\n",
    "    ),\n",
    "    # MLPClassifier(alpha=1, max_iter=100, random_state=42),\n",
    "    AdaBoostClassifier(algorithm=\"SAMME\", random_state=42),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "# Flattening the data for all channels\n",
    "def flatten_data(data_pad):\n",
    "    flattened_data = [sample.flatten().numpy() for sample in data_pad.values()]\n",
    "    return flattened_data\n",
    "\n",
    "X_train = flatten_data(train_data_pad)\n",
    "y_train = [output for output in train_outputs.values()]\n",
    "\n",
    "X_test = flatten_data(test_data_pad)\n",
    "y_test = [output for output in test_outputs.values()]\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print(name, file=sys.stderr)\n",
    "    clf = make_pipeline(StandardScaler(), clf)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(f\"{name}: {score}\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2e732-ef83-4fe8-9252-1e38370f9648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
