{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9b1a80-cd86-4ed3-b427-f038f1e0601a",
   "metadata": {},
   "source": [
    "# Finetuning Notebook for Thesis (Simple Classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f2d7d-b21d-4383-ac28-f6f3616c10aa",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec7e8af-477d-4d7a-b00d-4c6a7da090ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Third-party library imports\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmne\u001b[39;00m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/pandas/__init__.py:138\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28meval\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    122\u001b[0m     concat,\n\u001b[1;32m    123\u001b[0m     lreshape,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     qcut,\n\u001b[1;32m    136\u001b[0m )\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_print_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/pandas/api/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" public toolkit API \"\"\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     extensions,\n\u001b[1;32m      4\u001b[0m     indexers,\n\u001b[1;32m      5\u001b[0m     interchange,\n\u001b[1;32m      6\u001b[0m     types,\n\u001b[1;32m      7\u001b[0m     typing,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterchange\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextensions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtyping\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m ]\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/pandas/api/typing/__init__.py:31\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     Expanding,\n\u001b[1;32m     21\u001b[0m     ExpandingGroupby,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     Window,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# TODO: Can't import Styler without importing jinja2\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# from pandas.io.formats.style import Styler\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JsonReader\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StataReader\n\u001b[1;32m     34\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrameGroupBy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatetimeIndexResamplerGroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     55\u001b[0m ]\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/pandas/io/json/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     read_json,\n\u001b[1;32m      3\u001b[0m     to_json,\n\u001b[1;32m      4\u001b[0m     ujson_dumps,\n\u001b[1;32m      5\u001b[0m     ujson_loads,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_table_schema\n\u001b[1;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_dumps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_loads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_table_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m ]\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/pandas/io/json/_json.py:71\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_normalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_line_delimits\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     68\u001b[0m     build_table_schema,\n\u001b[1;32m     69\u001b[0m     parse_table_schema,\n\u001b[1;32m     70\u001b[0m )\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_integer\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     75\u001b[0m         Hashable,\n\u001b[1;32m     76\u001b[0m         Mapping,\n\u001b[1;32m     77\u001b[0m     )\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/pandas/io/parsers/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     TextFileReader,\n\u001b[1;32m      3\u001b[0m     TextParser,\n\u001b[1;32m      4\u001b[0m     read_csv,\n\u001b[1;32m      5\u001b[0m     read_fwf,\n\u001b[1;32m      6\u001b[0m     read_table,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextFileReader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextParser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_fwf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_table\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m using_copy_on_write\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STR_NA_VALUES\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     AbstractMethodError,\n\u001b[1;32m     35\u001b[0m     ParserWarning,\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Appender\n",
      "File \u001b[0;32mparsers.pyx:1418\u001b[0m, in \u001b[0;36minit pandas._libs.parsers\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")\n",
    "\n",
    "# Add custom path\n",
    "import sys\n",
    "sys.path.append(\"/home/maxihuber/eeg-foundation/\")\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import mne\n",
    "import lightning.pytorch as L\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, mean_squared_error\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# MNE imports\n",
    "from mne.preprocessing import Xdawn\n",
    "\n",
    "# Custom imports\n",
    "from src.utils.preloading.utils import load_edf_to_dataframe\n",
    "\n",
    "# Set MNE log level\n",
    "mne.set_log_level('warning')\n",
    "\n",
    "# Seed everything\n",
    "L.seed_everything(42)\n",
    "\n",
    "print(\"Bye world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea036b9-4979-4883-8acc-708017b99fb1",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3428f8de-eb21-492f-b8fd-a274c9a8d86a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load Train/Val/Test Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140a137e-6e06-483e-8ff6-f9722725fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# TUAB and Epilepsy\n",
    "\n",
    "yc_class = {\n",
    "    \"class_name\": \"YC\",\n",
    "    \"time_col\": \"Time in Seconds\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation/tueg/edf\",\n",
    "    \"load_mode\": 2,\n",
    "}\n",
    "\n",
    "tuab = {\n",
    "    \"task_name\": \"TUAB\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/tuab_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "epilepsy = {\n",
    "    \"task_name\": \"Epilepsy\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/epilepsy_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "yc_tasks = [tuab, epilepsy]\n",
    "\n",
    "########################################################################################################################\n",
    "# Clinical JSONs\n",
    "\n",
    "cli_class = {\n",
    "    \"class_name\": \"Clinical\",\n",
    "    \"time_col\": \"Time in Seconds\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_prepared/\",\n",
    "    \"load_mode\": 0,\n",
    "}\n",
    "\n",
    "age = {\n",
    "    \"task_name\": \"Age\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/age_light.json\",\n",
    "    \"out_dim\": 1,\n",
    "}\n",
    "\n",
    "depression = {\n",
    "    \"task_name\": \"Depression\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/cli_depression_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "parkinsons = {\n",
    "    \"task_name\": \"Parkinsons\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/cli_parkinsons_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "schizophrenia = {\n",
    "    \"task_name\": \"Schizophrenia\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/cli_schizophrenia_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "sex = {\n",
    "    \"task_name\": \"Sex\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/sex_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "cli_tasks = [age, depression, parkinsons, schizophrenia, sex]\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# Motor-Imagery JSONs\n",
    "\n",
    "mi_class = {\n",
    "    \"class_name\": \"Motor Imagery\",\n",
    "    \"time_col\": \"time in seconds\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation_prepared/\",\n",
    "    \"load_mode\": 0,\n",
    "}\n",
    "\n",
    "eye_open_closed = {\n",
    "    \"task_name\": \"EyeOpenClosed\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/eye_open_closed_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"eye open\", \"eye closed\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "eye_vh = {\n",
    "    \"task_name\": \"EyeVH\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/eye_vh_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"vertical\", \"horizontal\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "flexion_extension_imaginary = {\n",
    "    \"task_name\": \"FlexionExtensionImaginary\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/flexion_extension_imaginary_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"hand movement imagined elbow flexion\",\n",
    "            \"hand movement imagined elbow extension\",\n",
    "        ]\n",
    "    ),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "flexion_extension_real = {\n",
    "    \"task_name\": \"FlexionExtensionReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/flexion_extension_real_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"hand movement elbow extension\", \"hand movement elbow flexion\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "grasp_imaginary = {\n",
    "    \"task_name\": \"GraspImaginary\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/grasp_imaginary_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"imagined palmar grasp\", \"imagined lateral grasp\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "grasp_real = {\n",
    "    \"task_name\": \"GraspReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/grasp_real_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"movement palmar grasp\", \"movement lateral grasp\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "lr_imaginary = {\n",
    "    \"task_name\": \"LRImaginary\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/lr_imaginary_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"left hand imagined movement\", \"right hand imagined movement\"]),\n",
    "    \"short_mode\": True,\n",
    "}\n",
    "\n",
    "lr_real = {\n",
    "    \"task_name\": \"LRReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/lr_real_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"right hand movement\", \"left hand movement\"]),\n",
    "    \"short_mode\": True,\n",
    "}\n",
    "\n",
    "mi_task_body_parts_imagined = {\n",
    "    \"task_name\": \"BodyPartsImagined\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/mi_task_imagined_body_parts_light.json\",\n",
    "    \"out_dim\": 5,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"rest\",\n",
    "            \"right hand imagined movement\",\n",
    "            \"foot imagined movement\",\n",
    "            \"left hand imagined movement\",\n",
    "            \"tongue imagined movement\",\n",
    "        ]\n",
    "    ),\n",
    "    \"short_mode\": True,\n",
    "}\n",
    "\n",
    "mi_task_body_parts_real = {\n",
    "    \"task_name\": \"BodyPartsReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/mi_task_body_parts_light.json\",\n",
    "    \"out_dim\": 4,\n",
    "    \"outputs\": set(\n",
    "        [\"rest\", \"right hand movement\", \"foot movement\", \"left hand movement\"]\n",
    "    ),\n",
    "    \"short_mode\": True,\n",
    "}\n",
    "\n",
    "pronation_supination_imaginary = {\n",
    "    \"task_name\": \"PronationSupinationImaginary\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/pronation_supination_imaginary_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"imagined supination\", \"imagined pronation\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "pronation_supination_real = {\n",
    "    \"task_name\": \"PronationSupinationReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/pronation_supination_real_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"movement supination\", \"movement pronation\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "mi_tasks = [eye_open_closed, eye_vh, flexion_extension_imaginary, flexion_extension_real, \n",
    "            grasp_imaginary, grasp_real, lr_imaginary, lr_real,\n",
    "            mi_task_body_parts_imagined, mi_task_body_parts_real,\n",
    "            pronation_supination_imaginary, pronation_supination_real]\n",
    "\n",
    "########################################################################################################################\n",
    "# ERP JSONs\n",
    "\n",
    "erp_class = {\n",
    "    \"class_name\": \"Error-Related Potential\",\n",
    "    \"time_col\": \"time in seconds\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation_prepared/\",\n",
    "    \"load_mode\": 0,\n",
    "}\n",
    "\n",
    "erp = {\n",
    "    \"task_name\": \"ERP\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/new_erp_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"with event-related potential\",\n",
    "            \"without event-related potential\",\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "errp = {\n",
    "    \"task_name\": \"ERRP\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/net_scratch/finetune_files/errp_all_light.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"without error-related potential\",\n",
    "            \"with error-related potential\",\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "erp_tasks = [erp, errp]\n",
    "\n",
    "########################################################################################################################\n",
    "# EyeNet JSONs\n",
    "\n",
    "eye_class = {\n",
    "    \"class_name\": \"EyeNet\",\n",
    "    \"time_col\": \"time\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation_prepared/\",\n",
    "    \"load_mode\": 1,\n",
    "}\n",
    "\n",
    "eye_dir_amp = {\n",
    "    \"task_name\": \"EyeNetDirectionAmp\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": [\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Amp_train.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Amp_val.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Amp_test.json\",\n",
    "    ],\n",
    "    \"out_dim\": 1,\n",
    "}\n",
    "\n",
    "eye_dir_ang = {\n",
    "    \"task_name\": \"EyeNetDirectionAng\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": [\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Ang_train.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Ang_val.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Ang_test.json\",\n",
    "    ],\n",
    "    \"out_dim\": 1,\n",
    "}\n",
    "\n",
    "eye_lr = {\n",
    "    \"task_name\": \"EyeNetLR\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": [\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_LR_train.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_LR_val.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_LR_test.json\",\n",
    "    ],\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "eye_position = {\n",
    "    \"task_name\": \"EyeNetPosition\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": [\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Position_train.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Position_val.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Position_test.json\",\n",
    "    ],\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "eye_tasks = [eye_dir_amp, eye_dir_ang, eye_lr, eye_position]\n",
    "\n",
    "classes = {\n",
    "    \"YC\": [yc_class, yc_tasks], \n",
    "    \"Clinical\": [cli_class, cli_tasks], \n",
    "    \"MI\": [mi_class, mi_tasks],\n",
    "    \"ERP\": [erp_class, erp_tasks], \n",
    "    \"EyeNet\": [eye_class, eye_tasks],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b6468-0350-4044-99ba-3683577cd59b",
   "metadata": {},
   "source": [
    "### Load data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ba553-0c71-4133-9175-500d9b25fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Select the class and task\n",
    "\n",
    "# used_class = yc_class\n",
    "# used_class = cli_class\n",
    "used_class = mi_class\n",
    "# used_class = erp_class\n",
    "# used_class = eye_class\n",
    "#\n",
    "# used_task = tuab\n",
    "# used_task = epilepsy\n",
    "# used_task = age\n",
    "# used_task = depression\n",
    "# used_task = parkinsons\n",
    "# used_task = schizophrenia\n",
    "# used_task = sex\n",
    "#\n",
    "used_task = eye_open_closed\n",
    "# used_task = eye_vh\n",
    "# used_task = flexion_extension_imaginary\n",
    "# used_task = flexion_extension_real\n",
    "# used_task = grasp_real\n",
    "# used_task = lr_imaginary\n",
    "# used_task = lr_real\n",
    "# used_task = mi_task_body_parts_real\n",
    "# used_task = mi_task_body_parts_imagined\n",
    "# used_task = pronation_supination_real\n",
    "# used_task = pronation_supination_imaginary\n",
    "#\n",
    "# used_task = erp\n",
    "# used_task = errp\n",
    "#\n",
    "# used_task = eye_dir_amp\n",
    "# used_task = eye_dir_ang\n",
    "# used_task = eye_lr\n",
    "# used_task = eye_position\n",
    "\n",
    "class_name = used_class[\"class_name\"]\n",
    "time_col = used_class[\"time_col\"]\n",
    "prefix_filepath = used_class[\"prefix_filepath\"]\n",
    "load_mode = used_class[\"load_mode\"]\n",
    "task_name = used_task[\"task_name\"]\n",
    "task_type = used_task[\"task_type\"]\n",
    "json_path = used_task[\"json_path\"]\n",
    "out_dim = used_task[\"out_dim\"]\n",
    "short_mode = used_task[\"short_mode\"] if \"short_mode\" in used_task else False\n",
    "\n",
    "with open('/home/maxihuber/eeg-foundation/src/data/components/channels_to_id3.json', 'r') as f:\n",
    "    chn_to_id = json.load(f)\n",
    "    task_channels = set([chn: id for chn, id in chn_to_id.items() if id > 0]) # excludes \"None\"\n",
    "print(f\"Task channels: {task_channels}\")\n",
    "\n",
    "truncate = True\n",
    "num_keep = 100\n",
    "\n",
    "def load_index0(data_index_path):\n",
    "    with open(data_index_path, \"r\") as f:\n",
    "        train_test_dict = json.load(f)\n",
    "    train_samples = train_test_dict[\"train\"]\n",
    "    test_samples = train_test_dict[\"test\"]\n",
    "    return train_samples, test_samples\n",
    "\n",
    "def load_index1(data_index_paths):\n",
    "    all_samples = []\n",
    "    for data_index_path in data_index_paths:\n",
    "        with open(data_index_path, \"r\") as f:\n",
    "            subset_dict = json.load(f)\n",
    "        all_samples.append(list(subset_dict.values())[0])\n",
    "    return all_samples[0], all_samples[1], all_samples[2]\n",
    "\n",
    "def truncate0(train_index, test_index, num_keep, truncate=False):\n",
    "    train_index = train_index[:num_keep] + train_index[-num_keep:] if truncate else train_index\n",
    "    test_index = test_index[:num_keep] + test_index[-num_keep:] if truncate else test_index\n",
    "    return train_index, test_index\n",
    "\n",
    "def truncate1(train_index, val_index, test_index, num_keep, truncate=False):\n",
    "    train_index = train_index[:num_keep] + train_index[-num_keep:] if truncate else train_index\n",
    "    val_index = val_index[:num_keep] + val_index[-num_keep:] if truncate else val_index\n",
    "    test_index = test_index[:num_keep] + test_index[-num_keep:] if truncate else test_index\n",
    "    return train_index, val_index, test_index\n",
    "\n",
    "def get_node_index(index_patterns):\n",
    "    index_paths = []\n",
    "    for pattern in index_patterns:  # regex the index_patterns\n",
    "        index_paths.extend(glob.glob(pattern))\n",
    "    num_trials = 0\n",
    "    trial_info_index = {}\n",
    "    for index_path in index_paths:\n",
    "        with open(index_path, \"r\") as f:\n",
    "            new_trial_info_index = json.load(f)\n",
    "            for trial_info in new_trial_info_index.values():\n",
    "                trial_info_index[num_trials] = trial_info\n",
    "                num_trials += 1\n",
    "    print(f\"[get_node_index] # Trials = {num_trials}\", file=sys.stderr)\n",
    "    return trial_info_index\n",
    "\n",
    "def get_full_paths(input_files, prefix_filepath, filename_to_nodepath):\n",
    "    adjusted_files = []\n",
    "    for file in input_files:\n",
    "        if os.path.basepath(file) in filename_to_nodepath:\n",
    "            print(\"hit\")\n",
    "            adjusted_files.append(filename_to_nodepath[file])\n",
    "        else:\n",
    "            file = prefix_filepath + file if \"/itet-stor\" not in file else file.replace(\"/itet-stor/kard\", \"/itet-stor/maxihuber\")\n",
    "            adjusted_files.append(file)\n",
    "    return adjusted_files\n",
    "\n",
    "def get_generic_channel_name(channel_name):\n",
    "    channel_name = channel_name.lower()\n",
    "    # Remove \"eeg \" prefix if present\n",
    "    if channel_name.startswith(\"eeg \"):\n",
    "        channel_name = channel_name[4:]\n",
    "    # Simplify names with a dash and check if it ends with \"-\"\n",
    "    if \"-\" in channel_name:\n",
    "        if channel_name.endswith(\"-\"):\n",
    "            return \"None\"\n",
    "        return channel_name.split(\"-\")[0]\n",
    "    return channel_name\n",
    "\n",
    "def load_file_data(data_index, task_channels, filename_to_nodepath, load_mode, task_name):\n",
    "    num_samples = 0\n",
    "    data = {}\n",
    "    outputs = {}\n",
    "    srs = {}\n",
    "    durs = {}\n",
    "    channels = {}\n",
    "    failed_samples = []\n",
    "\n",
    "    for sample in tqdm(data_index, desc=\"Loading data\", position=0, leave=True):\n",
    "        try:\n",
    "            # Load the data of this sample\n",
    "            input_files = get_full_paths(sample[\"input\"], prefix_filepath, filename_to_nodepath)\n",
    "\n",
    "            if load_mode == 2:\n",
    "                file = input_files[0]\n",
    "                df = load_edf_to_dataframe(file)\n",
    "            else:\n",
    "                dataframes = [pd.read_pickle(file) for file in input_files]\n",
    "                df = pd.concat(dataframes, axis=0)\n",
    "            \n",
    "            # Crop the data to the desired length\n",
    "            start = int(sample[\"start\"])\n",
    "            length = int(sample[\"length\"]) if \"length\" in sample else int(sample[\"end\"])\n",
    "            df.loc[start : start + length, :] if load_mode==1 else df.iloc[start:length, :]\n",
    "            assert len(df) > 0, f\"Empty dataframe for sample: {sample}\"\n",
    "   \n",
    "            # Add labels\n",
    "            if load_mode != 1:\n",
    "                outputs[num_samples] = sample.get(\"output\", sample.get(\"label\"))\n",
    "            else:\n",
    "                outputs[num_samples] = list(sample[\"output\"].values()) if task_name == \"EyeNetPosition\" else list(sample[\"output\"].values())[0]\n",
    "            \n",
    "            # Add metadata\n",
    "            sr = int(1 / (df[time_col].iloc[1] - df[time_col].iloc[0]))\n",
    "            srs[num_samples] = sr\n",
    "            durs[num_samples] = len(df) / sr\n",
    "            \n",
    "            channels[num_samples] = sorted(list(set(df.columns) & set(task_channels)), key=lambda x: list(task_channels).index(x))\n",
    "            df = df[channels[num_samples]].astype(float)\n",
    "            data[num_samples] = torch.tensor(df.to_numpy(), dtype=torch.float32).T\n",
    "            \n",
    "            num_samples += 1\n",
    "            \n",
    "            del df\n",
    "            if num_samples % 100 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process sample: {sample}. Error: {e}\", file=sys.stderr)\n",
    "            failed_samples.append(sample)\n",
    "\n",
    "    return data, outputs, srs, durs, channels\n",
    "\n",
    "\n",
    "print(f\"Preparing local paths...\")\n",
    "index_patterns = [\"/dev/shm/mae/index_*.json\", \"/scratch/mae/index_*.json\"]\n",
    "node_index = get_node_index(index_patterns=index_patterns)\n",
    "filename_to_nodepath = {os.path.basename(ie[\"origin_path\"]): ie[\"new_path\"] for trial_idx, ie in node_index.items()}\n",
    "print(f\"Prepared local paths. {len(filename_to_nodepath)} files found on node.\")\n",
    "\n",
    "if load_mode != 1:\n",
    "    train_index, test_index = load_index0(json_path)\n",
    "    train_index, test_index = truncate0(train_index, test_index, num_keep, truncate)\n",
    "    \n",
    "    print(\"=\" * 10 + \"Load train data\" + \"=\" * 100)\n",
    "    train_data, train_outputs, train_sr, train_dur, train_channels = (\n",
    "        load_file_data(train_index, task_channels, filename_to_nodepath, load_mode, task_name)\n",
    "    )\n",
    "    print(\"=\" * 10 + \"Load test data\" + \"=\" * 100)\n",
    "    test_data, test_outputs, test_sr, test_dur, test_channels = (\n",
    "        load_file_data(test_index, task_channels, filename_to_nodepath, load_mode, task_name)\n",
    "    )\n",
    "else:\n",
    "    train_index, val_index, test_index = load_index1(json_path)\n",
    "    train_index, val_index, test_index = truncate1(train_index, val_index, test_index, num_keep, truncate)\n",
    "    \n",
    "    print(\"=\" * 10 + \"Load train data\" + \"=\" * 100)\n",
    "    train_data, train_outputs, train_sr, train_dur, train_channels = (\n",
    "        load_file_data(train_index, task_channels, filename_to_nodepath, load_mode, task_name)\n",
    "    )\n",
    "    print(\"=\" * 10 + \"Load val data\" + \"=\" * 100)\n",
    "    val_data, val_outputs, val_sr, val_dur, val_channels = load_file_data(\n",
    "        val_index, task_channels, filename_to_nodepath, load_mode, task_name\n",
    "    )\n",
    "    print(\"=\" * 10 + \"Load test data\" + \"=\" * 100)\n",
    "    test_data, test_outputs, test_sr, test_dur, test_channels = (\n",
    "        load_file_data(test_index, task_channels, filename_to_nodepath, load_mode, task_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d7502a-6b61-4828-bf1c-e7b8f5cddc66",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f65a6-9ecf-4f1a-89cd-7ac4d06c8757",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc108915",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4ad85-e544-4be1-8061-a1dd64d985c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(tensor, target_height, target_width):\n",
    "    current_height, current_width = tensor.shape\n",
    "\n",
    "    if current_height < target_height:\n",
    "        padding_height = target_height - current_height\n",
    "        padding = torch.zeros((padding_height, current_width), dtype=tensor.dtype)\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "    else:\n",
    "        tensor = tensor[:target_height, :]\n",
    "\n",
    "    if current_width < target_width:\n",
    "        padding_width = target_width - current_width\n",
    "        padding = torch.zeros((tensor.shape[0], padding_width), dtype=tensor.dtype)\n",
    "        tensor = torch.cat((tensor, padding), dim=1)\n",
    "    else:\n",
    "        tensor = tensor[:, :target_width]\n",
    "\n",
    "    return tensor\n",
    "\n",
    "def resample_signals(data, srs, target_sfreq):\n",
    "    resampled_data = {}\n",
    "    for idx, signal in tqdm(data.items(), desc=\"Resampling signals\"):\n",
    "        signal_numpy = signal.numpy().astype(np.float64)\n",
    "        signal_resampled = mne.filter.resample(signal_numpy, up=target_sfreq / srs[idx])\n",
    "        resampled_data[idx] = torch.tensor(signal_resampled, dtype=torch.float32)\n",
    "        del signal_numpy, signal_resampled  # Free memory\n",
    "        gc.collect()  # Explicitly invoke garbage collection\n",
    "    return resampled_data\n",
    "\n",
    "def pad_or_truncate_signals(data, common_length):\n",
    "    for idx, signal in tqdm(data.items(), desc=\"Pad/Truncate signals\"):\n",
    "        signal_length = signal.shape[1]\n",
    "        if signal_length < common_length:\n",
    "            pad_width = common_length - signal_length\n",
    "            signal_padded = np.pad(signal, ((0, 0), (0, pad_width)), mode=\"constant\")\n",
    "        else:\n",
    "            signal_padded = signal[:, :common_length]\n",
    "        data[idx] = torch.tensor(signal_padded, dtype=torch.float32)\n",
    "        del signal, signal_padded  # Free memory\n",
    "        gc.collect()  # Explicitly invoke garbage collection\n",
    "    return data\n",
    "\n",
    "def create_epochs(data, outputs, channels, sfreq=1000, is_classification=True):\n",
    "    events = []\n",
    "    event_id = {}\n",
    "    epochs_data = []\n",
    "    for idx, signal in tqdm(data.items(), desc=\"Creating epochs\"):\n",
    "        epochs_data.append(signal.numpy())\n",
    "        if is_classification:\n",
    "            if outputs[idx] not in event_id:\n",
    "                event_id[outputs[idx]] = len(event_id) + 1\n",
    "            events.append([idx, 0, event_id[outputs[idx]]])\n",
    "        else:\n",
    "            events.append([idx, 0, 1])\n",
    "    events = np.array(events, dtype=int)\n",
    "    info = mne.create_info(\n",
    "        ch_names=[f\"EEG_{i}\" for i in range(channels[0].shape[0])], sfreq=sfreq, ch_types=\"eeg\"\n",
    "    )\n",
    "    epochs = mne.EpochsArray(\n",
    "        np.array(epochs_data),\n",
    "        info,\n",
    "        events=events,\n",
    "        event_id=event_id if is_classification else None,\n",
    "    )\n",
    "    del events, info, epochs_data  # Free memory\n",
    "    gc.collect()  # Explicitly invoke garbage collection\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387a9b7-5b7a-45ad-9f9e-4136aa5f3541",
   "metadata": {},
   "source": [
    "### Applying xDAWN Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea15d2-0db4-448e-b1dc-90e0206b4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(42)\n",
    "sys.path.append(\"/home/maxihuber/eeg-foundation/src/models/components/Baselines\")\n",
    "\n",
    "os.makedirs(f\"/itet-stor/maxihuber/net_scratch/finetune_ckpts/{task_name}\", exist_ok=True)\n",
    "\n",
    "durs = [df.shape[1] for idx, df in train_data.items()] + [df.shape[1] for idx, df in test_data.items()]\n",
    "n_chns = [df.shape[0] for idx, df in train_data.items()] + [df.shape[0] for idx, df in test_data.items()]\n",
    "dur_90 = int(np.percentile(durs, 90))\n",
    "chn_90 = int(np.percentile(durs, 90))\n",
    "\n",
    "train_data_pad = {k: pad_tensor(signals, chn_90, dur_90) for k, signals in train_data.items()}\n",
    "test_data_pad = {k: pad_tensor(signals, chn_90, dur_90) for k, signals in test_data.items()}\n",
    "del train_data, test_data  # Free memory\n",
    "gc.collect()  # Explicitly invoke garbage collection\n",
    "\n",
    "target_sfreq = int(max(train_sr.values()))\n",
    "\n",
    "common_length = min(\n",
    "    min(signal.shape[1] for signal in train_data_pad.values()),\n",
    "    min(signal.shape[1] for signal in test_data_pad.values()),\n",
    ")\n",
    "\n",
    "train_data_padded = pad_or_truncate_signals(train_data_pad, common_length)\n",
    "test_data_padded = pad_or_truncate_signals(test_data_pad, common_length)\n",
    "del train_data_pad, test_data_pad  # Free memory\n",
    "gc.collect()  # Explicitly invoke garbage collection\n",
    "\n",
    "is_classification = True if task_type == \"Classification\" else False\n",
    "epochs_train = create_epochs(\n",
    "    train_data_padded, train_outputs, list(train_channels.values()), target_sfreq, is_classification\n",
    ")\n",
    "epochs_test = create_epochs(\n",
    "    test_data_padded, test_outputs, list(test_channels.values()), target_sfreq, is_classification\n",
    ")\n",
    "del train_data_padded, test_data_padded  # Free memory\n",
    "gc.collect()  # Explicitly invoke garbage collection\n",
    "\n",
    "print(\"Start fitting Xdawn...\", file=sys.stderr)\n",
    "\n",
    "xdawn = Xdawn(n_components=2, correct_overlap=False, reg=0.1)\n",
    "xdawn.fit(epochs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778e0ab-d9b2-444e-8f8a-ebd4b2ff8ceb",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e10e692-f09f-499a-bcef-a74abba07cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "# https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "def get_labels(train_outputs, test_outputs, is_classification):\n",
    "    if is_classification:\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train = label_encoder.fit_transform(list(train_outputs.values()))\n",
    "        y_test = label_encoder.transform(list(test_outputs.values()))\n",
    "    else:\n",
    "        y_train = np.array(list(train_outputs.values()))\n",
    "        y_test = np.array(list(test_outputs.values()))\n",
    "    return y_train, y_test\n",
    "\n",
    "# TODO: check which classification & regression models we actually want\n",
    "simple_models = {\n",
    "    \"Classification\": {\n",
    "        \"LDA\": LinearDiscriminantAnalysis(),\n",
    "        \"QDA\": QuadraticDiscriminantAnalysis(),\n",
    "        \"Nearest Neighbors\": KNeighborsClassifier(3),\n",
    "        \"Linear SVM\": SVC(kernel=\"linear\", C=0.025, random_state=42),\n",
    "        \"RBF SVM\": SVC(gamma=2, C=1, random_state=42),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(\n",
    "            max_depth=5, n_estimators=10, max_features=1, random_state=42\n",
    "        ),\n",
    "        \"AdaBoost\": AdaBoostClassifier(algorithm=\"SAMME\", random_state=42),\n",
    "    },\n",
    "    \"Regression\": {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "        \"K-Nearest Neighbors Regressor\": KNeighborsRegressor(n_neighbors=3),\n",
    "        \"Support Vector Regression (Linear)\": SVR(kernel='linear', C=1.0),\n",
    "        \"Support Vector Regression (RBF)\": SVR(kernel='rbf', C=1.0, gamma=0.1),\n",
    "        \"Decision Tree Regressor\": DecisionTreeRegressor(max_depth=5, random_state=42),\n",
    "        \"Random Forest Regressor\": RandomForestRegressor(\n",
    "            max_depth=5, n_estimators=10, max_features=1, random_state=42\n",
    "        ),\n",
    "        \"AdaBoost Regressor\": AdaBoostRegressor(n_estimators=50, random_state=42),\n",
    "    },\n",
    "}\n",
    "\n",
    "# TODO: more accuracy parameters, for both regression and classification\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, mean_squared_error, mean_absolute_error,\n",
    "    r2_score, mean_absolute_percentage_error\n",
    ")\n",
    "scores = {\n",
    "    \"Classification\": {\n",
    "        \"Accuracy\": accuracy_score,\n",
    "        \"Balanced Accuracy\": balanced_accuracy_score,\n",
    "        \"Precision\": precision_score,\n",
    "        \"Recall\": recall_score,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"ROC AUC\": roc_auc_score,\n",
    "        \"Confusion Matrix\": confusion_matrix,\n",
    "    },\n",
    "    \"Regression\": {\n",
    "        \"MAE\": mean_absolute_error,\n",
    "        \"RMSE\": lambda y_true, y_pred: mean_squared_error(y_true, y_pred, squared=False),  # Using lambda for RMSE\n",
    "        \"R-squared\": r2_score,\n",
    "        \"MAPE\": mean_absolute_percentage_error,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Transform the data using xDAWN\n",
    "X_train_xdawn = xdawn.transform(epochs_train)\n",
    "X_test_xdawn = xdawn.transform(epochs_test)\n",
    "\n",
    "# Flatten the transformed data for LDA input\n",
    "n_epochs_train, n_components, n_times = X_train_xdawn.shape\n",
    "X_train_xdawn = X_train_xdawn.reshape(n_epochs_train, n_components * n_times)\n",
    "n_epochs_test, n_components, n_times = X_test_xdawn.shape\n",
    "X_test_xdawn = X_test_xdawn.reshape(n_epochs_test, n_components * n_times)\n",
    "\n",
    "y_train, y_test = get_labels(train_outputs, test_outputs, is_classification)\n",
    "\n",
    "for name, clf in simple_models[task_type].items():\n",
    "    print(f\"Evaluating {name} model.\", file=sys.stderr)\n",
    "    \n",
    "    clf_pipeline = make_pipeline(StandardScaler(), clf)\n",
    "    clf_pipeline.fit(X_train_xdawn, y_train)\n",
    "    y_pred = clf_pipeline.predict(X_test_xdawn)\n",
    "\n",
    "    for score_name, score_func in scores[task_type].items():\n",
    "        score = score_func(y_test, y_pred)\n",
    "        print(f\"{score_name}: {score} ({name})\", file=sys.stderr)\n",
    "\n",
    "    # Clean up to save memory\n",
    "    del clf_pipeline, y_pred\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
