{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "import glob\n",
    "import gc\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"/home/maxihuber/eeg-foundation\")\n",
    "from src.utils.preloading.utils import load_edf_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Target is located in the right', 'without error-related potential', 'The cursor moves to the left', 'with error-related potential', 'The feedback consisted in the selected item is presented on the screen', 'The cursor moves to the right', 'Target is located in the left'}\n"
     ]
    }
   ],
   "source": [
    "erp = {\n",
    "    \"task_name\": \"ERP\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/erp/new_erp.json\",\n",
    "    \"out_dim\": 5,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"Participant is in resting state\",\n",
    "            \"with event-related potential\",\n",
    "            \"Participant is in interval between two flashes\",\n",
    "            \"without event-related potential\",\n",
    "            \"Participant keeps closing eyes\",\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "errp = {\n",
    "    \"task_name\": \"ERRP\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/erp/errp_all.json\",\n",
    "    \"out_dim\": 7,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"Target is located in the right\",\n",
    "            \"without error-related potential\",\n",
    "            \"The cursor moves to the left\",\n",
    "            \"The feedback consisted in the selected item is presented on the screen\",\n",
    "            \"The cursor moves to the right\",\n",
    "            \"with error-related potential\",\n",
    "            \"Target is located in the left\",\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "with open(errp[\"json_path\"], \"r\") as f:\n",
    "    train_test_data = json.load(f)\n",
    "\n",
    "outputs = set([sample[\"label\"] for sample in train_test_data[\"train\"]])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "#Â TUAB and Epilepsy\n",
    "\n",
    "yc_class = {\n",
    "    \"class_name\": \"YC\",\n",
    "    \"time_col\": \"Time in Seconds\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation/tueg/edf\",\n",
    "    \"load_mode\": 2,\n",
    "}\n",
    "\n",
    "tuab = {\n",
    "    \"task_name\": \"TUAB\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/cli/tuab.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "epilepsy = {\n",
    "    \"task_name\": \"Epilepsy\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/cli/epilepsy.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "yc_tasks = [tuab, epilepsy]\n",
    "\n",
    "########################################################################################################################\n",
    "# Clinical JSONs\n",
    "\n",
    "cli_class = {\n",
    "    \"class_name\": \"Clinical\",\n",
    "    \"time_col\": \"Time in Seconds\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_prepared/\",\n",
    "    \"load_mode\": 0,\n",
    "}\n",
    "\n",
    "age = {\n",
    "    \"task_name\": \"Age\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/age2.json\",\n",
    "    \"out_dim\": 1,\n",
    "}\n",
    "\n",
    "depression = {\n",
    "    \"task_name\": \"Depression\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/cli_depression.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "parkinsons = {\n",
    "    \"task_name\": \"Parkinsons\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/parkinsons2.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "schizophrenia = {\n",
    "    \"task_name\": \"Schizophrenia\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/cli_schizophrenia.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "sex = {\n",
    "    \"task_name\": \"Sex\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/sex2.json\",\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "cli_tasks = [age, depression, parkinsons, schizophrenia, sex]\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# Motor-Imagery JSONs\n",
    "\n",
    "mi_class = {\n",
    "    \"class_name\": \"Motor Imagery\",\n",
    "    \"time_col\": \"time in seconds\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation_prepared/\",\n",
    "    \"load_mode\": 0,\n",
    "}\n",
    "\n",
    "eye_open_closed = {\n",
    "    \"task_name\": \"EyeOpenClosed\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/eye_open_closed.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"eye open\", \"eye closed\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "eye_vh = {\n",
    "    \"task_name\": \"EyeVH\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/eye_vh.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"vertical\", \"horizontal\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "flexion_extension_imaginary = {\n",
    "    \"task_name\": \"FlexionExtensionImaginary\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/flexion_extension_imaginary.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"hand movement imagined elbow flexion\",\n",
    "            \"hand movement imagined elbow extension\",\n",
    "        ]\n",
    "    ),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "flexion_extension_real = {\n",
    "    \"task_name\": \"FlexionExtensionReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/flexion_extension_real.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"hand movement elbow extension\", \"hand movement elbow flexion\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "grasp_imaginary = {\n",
    "    \"task_name\": \"GraspImaginary\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/grasp_imaginary.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"imagined palmar grasp\", \"imagined lateral grasp\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "grasp_real = {\n",
    "    \"task_name\": \"GraspReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/grasp_real.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"movement palmar grasp\", \"movement lateral grasp\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "lr_imaginary = {\n",
    "    \"task_name\": \"LRImaginary\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/lr_imaginary.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"left hand imagined movement\", \"right hand imagined movement\"]),\n",
    "    \"short_mode\": True,\n",
    "}\n",
    "\n",
    "lr_real = {\n",
    "    \"task_name\": \"LRReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/lr_real.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"right hand movement\", \"left hand movement\"]),\n",
    "    \"short_mode\": True,\n",
    "}\n",
    "\n",
    "mi_task_body_parts_imagined = {\n",
    "    \"task_name\": \"BodyPartsImagined\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/mi_task_imagined_body_parts.json\",\n",
    "    \"out_dim\": 4,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"rest\",\n",
    "            \"right hand imagined movement\",\n",
    "            \"foot imagined movement\",\n",
    "            \"left hand imagined movement\",\n",
    "            \"tongue imagined movement\",\n",
    "        ]\n",
    "    ),\n",
    "    \"short_mode\": True,\n",
    "}\n",
    "\n",
    "mi_task_body_parts_real = {\n",
    "    \"task_name\": \"BodyPartsReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/mi_task_body_parts.json\",\n",
    "    \"out_dim\": 4,\n",
    "    \"outputs\": set(\n",
    "        [\"rest\", \"right hand movement\", \"foot movement\", \"left hand movement\"]\n",
    "    ),\n",
    "    \"short_mode\": True,\n",
    "}\n",
    "\n",
    "pronation_supination_imaginary = {\n",
    "    \"task_name\": \"PronationSupinationImaginary\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/pronation_supination_imaginary.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"imagined supination\", \"imagined pronation\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "pronation_supination_real = {\n",
    "    \"task_name\": \"PronationSupinationReal\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/pronation_supination_real.json\",\n",
    "    \"out_dim\": 2,\n",
    "    \"outputs\": set([\"movement supination\", \"movement pronation\"]),\n",
    "    \"short_mode\": False,\n",
    "}\n",
    "\n",
    "mi_tasks = [eye_open_closed, eye_vh, flexion_extension_imaginary, flexion_extension_real, \n",
    "            grasp_imaginary, grasp_real, lr_imaginary, lr_real,\n",
    "            mi_task_body_parts_imagined, mi_task_body_parts_real,\n",
    "            pronation_supination_imaginary, pronation_supination_real]\n",
    "\n",
    "########################################################################################################################\n",
    "# ERP JSONs\n",
    "\n",
    "erp_class = {\n",
    "    \"class_name\": \"Error-Related Potential\",\n",
    "    \"time_col\": \"time in seconds\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation_prepared/\",\n",
    "    \"load_mode\": 0,\n",
    "}\n",
    "\n",
    "erp = {\n",
    "    \"task_name\": \"ERP\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/erp/new_erp.json\",\n",
    "    \"out_dim\": 5,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"Participant is in resting state\",\n",
    "            \"with event-related potential\",\n",
    "            \"Participant is in interval between two flashes\",\n",
    "            \"without event-related potential\",\n",
    "            \"Participant keeps closing eyes\",\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "errp = {\n",
    "    \"task_name\": \"ERRP\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_tasks/erp/errp_all.json\",\n",
    "    \"out_dim\": 7,\n",
    "    \"outputs\": set(\n",
    "        [\n",
    "            \"Target is located in the right\",\n",
    "            \"without error-related potential\",\n",
    "            \"The cursor moves to the left\",\n",
    "            \"The feedback consisted in the selected item is presented on the screen\",\n",
    "            \"The cursor moves to the right\",\n",
    "            \"with error-related potential\",\n",
    "            \"Target is located in the left\",\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "erp_tasks = [erp, errp]\n",
    "\n",
    "########################################################################################################################\n",
    "# EyeNet JSONs\n",
    "\n",
    "eye_class = {\n",
    "    \"class_name\": \"EyeNet\",\n",
    "    \"time_col\": \"time\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation_prepared/\",\n",
    "    \"load_mode\": 1,\n",
    "}\n",
    "\n",
    "eye_dir_amp = {\n",
    "    \"task_name\": \"EyeNetDirectionAmp\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": [\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Amp_train.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Amp_val.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Amp_test.json\",\n",
    "    ],\n",
    "    \"out_dim\": 1,\n",
    "}\n",
    "\n",
    "eye_dir_ang = {\n",
    "    \"task_name\": \"EyeNetDirectionAng\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": [\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Ang_train.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Ang_val.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Ang_test.json\",\n",
    "    ],\n",
    "    \"out_dim\": 1,\n",
    "}\n",
    "\n",
    "eye_lr = {\n",
    "    \"task_name\": \"EyeNetLR\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": [\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_LR_train.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_LR_val.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_LR_test.json\",\n",
    "    ],\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "eye_position = {\n",
    "    \"task_name\": \"EyeNetPosition\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": [\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Position_train.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Position_val.json\",\n",
    "        \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Position_test.json\",\n",
    "    ],\n",
    "    \"out_dim\": 2,\n",
    "}\n",
    "\n",
    "eye_tasks = [eye_dir_amp, eye_dir_ang, eye_lr, eye_position]\n",
    "\n",
    "classes = {\n",
    "    \"YC\": [yc_class, yc_tasks],\n",
    "    \"Clinical\": [cli_class, cli_tasks], \n",
    "    \"MI\": [mi_class, mi_tasks],\n",
    "    \"ERP\": [erp_class, erp_tasks], \n",
    "    \"EyeNet\": [eye_class, eye_tasks],\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/itet-stor/maxihuber/net_scratch/finetune_files/tuab.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/cli/tuab.json to /itet-stor/maxihuber/net_scratch/finetune_files/tuab.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/epilepsy.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/cli/epilepsy.json to /itet-stor/maxihuber/net_scratch/finetune_files/epilepsy.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/eye_open_closed.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/eye_open_closed.json to /itet-stor/maxihuber/net_scratch/finetune_files/eye_open_closed.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/eye_vh.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/eye_vh.json to /itet-stor/maxihuber/net_scratch/finetune_files/eye_vh.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/flexion_extension_imaginary.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/flexion_extension_imaginary.json to /itet-stor/maxihuber/net_scratch/finetune_files/flexion_extension_imaginary.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/flexion_extension_real.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/flexion_extension_real.json to /itet-stor/maxihuber/net_scratch/finetune_files/flexion_extension_real.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/grasp_imaginary.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/grasp_imaginary.json to /itet-stor/maxihuber/net_scratch/finetune_files/grasp_imaginary.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/grasp_real.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/grasp_real.json to /itet-stor/maxihuber/net_scratch/finetune_files/grasp_real.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/lr_imaginary.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/lr_imaginary.json to /itet-stor/maxihuber/net_scratch/finetune_files/lr_imaginary.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/lr_real.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/lr_real.json to /itet-stor/maxihuber/net_scratch/finetune_files/lr_real.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/mi_task_imagined_body_parts.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/mi_task_imagined_body_parts.json to /itet-stor/maxihuber/net_scratch/finetune_files/mi_task_imagined_body_parts.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/mi_task_body_parts.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/mi_task_body_parts.json to /itet-stor/maxihuber/net_scratch/finetune_files/mi_task_body_parts.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/pronation_supination_imaginary.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/pronation_supination_imaginary.json to /itet-stor/maxihuber/net_scratch/finetune_files/pronation_supination_imaginary.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/pronation_supination_real.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/mi/pronation_supination_real.json to /itet-stor/maxihuber/net_scratch/finetune_files/pronation_supination_real.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/new_erp.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/erp/new_erp.json to /itet-stor/maxihuber/net_scratch/finetune_files/new_erp.json\n",
      "/itet-stor/maxihuber/net_scratch/finetune_files/errp_all.json\n",
      "Copied /itet-stor/maxihuber/deepeye_storage/foundation_tasks/erp/errp_all.json to /itet-stor/maxihuber/net_scratch/finetune_files/errp_all.json\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "classes_to_copy = {\n",
    "    \"YC\": [yc_class, yc_tasks],\n",
    "    \"MI\": [mi_class, mi_tasks],\n",
    "    \"ERP\": [erp_class, erp_tasks], \n",
    "}\n",
    "\n",
    "base_path = '/itet-stor/maxihuber/net_scratch/finetune_files/'\n",
    "\n",
    "for class_name, [class_dict, tasks] in classes_to_copy.items():\n",
    "    for task in tasks:\n",
    "        json_path = task[\"json_path\"]\n",
    "        json_path_new = os.path.join(base_path, os.path.basename(json_path))\n",
    "        print(json_path_new)\n",
    "        \n",
    "        # Copy the file from json_path to json_path_new\n",
    "        shutil.copy(json_path, json_path_new)\n",
    "        print(f'Copied {json_path} to {json_path_new}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index0(data_index_path):\n",
    "    with open(data_index_path, \"r\") as f:\n",
    "        train_test_dict = json.load(f)\n",
    "    train_samples = train_test_dict[\"train\"]\n",
    "    test_samples = train_test_dict[\"test\"]\n",
    "    return train_samples, test_samples\n",
    "\n",
    "def load_index1(data_index_paths):\n",
    "    all_samples = []\n",
    "    for data_index_path in data_index_paths:\n",
    "        with open(data_index_path, \"r\") as f:\n",
    "            subset_dict = json.load(f)\n",
    "        all_samples.append(list(subset_dict.values())[0])\n",
    "    return all_samples[0], all_samples[1], all_samples[2]\n",
    "\n",
    "def get_generic_channel_name(channel_name):\n",
    "    channel_name = channel_name.lower()\n",
    "    # Remove \"eeg \" prefix if present\n",
    "    if channel_name.startswith(\"eeg \"):\n",
    "        channel_name = channel_name[4:]\n",
    "    # Simplify names with a dash and check if it ends with \"-\"\n",
    "    if \"-\" in channel_name:\n",
    "        if channel_name.endswith(\"-\"):\n",
    "            return \"None\"\n",
    "        return channel_name.split(\"-\")[0]\n",
    "    return channel_name\n",
    "\n",
    "def get_node_index(index_patterns):\n",
    "    index_paths = []\n",
    "    for pattern in index_patterns:  # regex the index_patterns\n",
    "        index_paths.extend(glob.glob(pattern))\n",
    "    num_trials = 0\n",
    "    trial_info_index = {}\n",
    "    for index_path in index_paths:\n",
    "        with open(index_path, \"r\") as f:\n",
    "            new_trial_info_index = json.load(f)\n",
    "            for trial_info in new_trial_info_index.values():\n",
    "                trial_info_index[num_trials] = trial_info\n",
    "                num_trials += 1\n",
    "    print(f\"[get_node_index] # Trials = {num_trials}\", file=sys.stderr)\n",
    "    return trial_info_index\n",
    "\n",
    "def get_full_paths(input_files, prefix_filepath):\n",
    "    adjusted_files = []\n",
    "    for file in input_files:\n",
    "        if file in filename_to_nodepath:\n",
    "            adjusted_files.append(filename_to_nodepath[file])\n",
    "        else:\n",
    "            file = prefix_filepath + file if \"/itet-stor\" not in file else file.replace(\"/itet-stor/kard\", \"/itet-stor/maxihuber\")\n",
    "            adjusted_files.append(file)\n",
    "    return adjusted_files\n",
    "\n",
    "def get_size(data_index):\n",
    "    try:\n",
    "        sz = 0\n",
    "        for sample in tqdm(data_index, desc=\"Estimating file size\", position=0, leave=True):\n",
    "            input_files = get_full_paths(sample[\"input\"], prefix_filepath)\n",
    "            # get file size in bytes combined for all input_files (raw filepaths)\n",
    "            sz += sum([os.path.getsize(file) for file in input_files])\n",
    "    except Exception as e:\n",
    "            print(f\"Failed to process sample: {sample}. Error: {e}\", file=sys.stderr)\n",
    "    return sz\n",
    "\n",
    "def get_index_size(data_index, prefix_filepath):\n",
    "    sz = 0\n",
    "    for sample in data_index:\n",
    "        try:\n",
    "            input_files = get_full_paths(sample[\"input\"], prefix_filepath)\n",
    "            sz += sum([os.path.getsize(file) for file in input_files])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process sample: {sample}. Error: {e}\", file=sys.stderr)\n",
    "    return sz\n",
    "    \n",
    "\n",
    "def format_size(size_bytes):\n",
    "    # Helper function to convert bytes to MB, GB, etc.\n",
    "    if size_bytes == 0:\n",
    "        return \"0B\"\n",
    "    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\")\n",
    "    i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "    p = math.pow(1024, i)\n",
    "    s = round(size_bytes / p, 2)\n",
    "    return f\"{s} {size_name[i]}\"\n",
    "\n",
    "def get_channels(data_index, task_channels):\n",
    "    files_channels = []\n",
    "    channel_set = set()\n",
    "    for sample in tqdm(data_index, desc=\"Getting channels\", position=0, leave=True):\n",
    "        input_files = get_full_paths(sample[\"input\"])\n",
    "        file = input_files[0]\n",
    "        if load_mode == 2:\n",
    "            df = load_edf_to_dataframe(file)\n",
    "        else:\n",
    "            df = pd.DataFrame()\n",
    "            with open(file, \"rb\") as f:\n",
    "                df_new = pd.read_pickle(f)\n",
    "                df = pd.concat([df, df_new], axis=0)\n",
    "        channels = natsorted([get_generic_channel_name(ch) for ch in df.columns])\n",
    "        files_channels.append(channels)\n",
    "        # channel_set.update(channels)\n",
    "        del df\n",
    "        break\n",
    "    return files_channels\n",
    "\n",
    "def load_file_data(data_index, task_channels):\n",
    "    num_samples = 0\n",
    "    data = {}\n",
    "    outputs = {}\n",
    "    srs = {}\n",
    "    durs = {}\n",
    "    channels = {}\n",
    "    datasets = {}\n",
    "    failed_samples = []\n",
    "\n",
    "    for sample in tqdm(data_index, desc=\"Loading data\", position=0, leave=True):\n",
    "        try:\n",
    "            # Load and concatenate dataframe\n",
    "            input_files = get_full_paths(sample[\"input\"])\n",
    "\n",
    "            if load_mode == 2:\n",
    "                df = load_edf_to_dataframe(file)\n",
    "            else:\n",
    "                df = pd.DataFrame()\n",
    "                for file in input_files:\n",
    "                    if load_mode != 1:\n",
    "                        file = prefix_filepath + file\n",
    "                    else:\n",
    "                        file = file.replace(\"/itet-stor/kard\", \"/itet-stor/maxihuber\")                       \n",
    "                    with open(file, \"rb\") as f:\n",
    "                        df_new = pd.read_pickle(f)\n",
    "                        df = pd.concat([df, df_new], axis=0)\n",
    "\n",
    "            start = int(sample[\"start\"])\n",
    "            length = int(sample[\"length\"]) if \"length\" in sample else int(sample[\"end\"])\n",
    "            if load_mode != 1:\n",
    "                df = df.iloc[start:length, :]\n",
    "                if short_mode:\n",
    "                    df = df.iloc[: int(len(df) * 0.5), :]\n",
    "            else:\n",
    "                df = df.loc[start : start + length, :]\n",
    "\n",
    "            # Add metadata\n",
    "            if len(df) <= 1:\n",
    "                assert False\n",
    "            sr = int(\n",
    "                1 / float(float(df[time_col].iloc[1]) - float(df[time_col].iloc[0]))\n",
    "            )\n",
    "            if load_mode != 1:\n",
    "                outputs[num_samples] = (\n",
    "                    sample[\"output\"] if \"output\" in sample else sample[\"label\"]\n",
    "                )\n",
    "            else:\n",
    "                if task_name == \"EyeNetPosition\":\n",
    "                    outputs[num_samples] = list(sample[\"output\"].values())\n",
    "                else:\n",
    "                    outputs[num_samples] = list(sample[\"output\"].values())[0]\n",
    "            srs[num_samples] = sr\n",
    "            durs[num_samples] = len(df) / sr\n",
    "            channels[num_samples] = sorted(list(set(df.columns) & set(task_channels)), key=lambda x: list(task_channels).index(x))\n",
    "            df = df[channels[num_samples]].astype(float)\n",
    "            signals = torch.tensor(df.to_numpy(), dtype=torch.float32).T\n",
    "            data[num_samples] = signals\n",
    "            num_samples += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process sample: {sample}. Error: {e}\", file=sys.stderr)\n",
    "            failed_samples.append(sample)\n",
    "\n",
    "    return data, outputs, srs, durs, channels, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[get_node_index] # Trials = 216556\n"
     ]
    }
   ],
   "source": [
    "tueg = '/itet-stor/maxihuber/deepeye_storage/index_files/full_tueg_index2.json'\n",
    "pkl = '/itet-stor/maxihuber/deepeye_storage/index_files/full_pkl_index.json'\n",
    "\n",
    "with open(tueg, 'r') as f:\n",
    "    tueg = json.load(f)\n",
    "\n",
    "with open(pkl, 'r') as f:\n",
    "    pkl = json.load(f)\n",
    "\n",
    "path_to_ie = {os.path.basename(ie[\"path\"]): ie for ie in (tueg + pkl)}\n",
    "\n",
    "task_channels = '/home/maxihuber/eeg-foundation/src/data/components/channels_to_id3.json'\n",
    "with open(task_channels, 'r') as f:\n",
    "    task_channels = set(json.load(f).keys())\n",
    "\n",
    "index_patterns = [\"/dev/shm/mae/index_*.json\", \"/scratch/mae/index_*.json\"]\n",
    "node_index = get_node_index(index_patterns=index_patterns)\n",
    "filename_to_nodepath = {os.path.basename(ie[\"origin_path\"]): ie[\"new_path\"] for trial_idx, ie in node_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_fetch_channels(used_class, used_task, task_channels):\n",
    "    if load_mode != 1:\n",
    "        train_index, test_index = load_index0(json_path)\n",
    "    else:\n",
    "        train_index, val_index, test_index = load_index1(json_path)\n",
    "\n",
    "    if load_mode == 0 or load_mode == 2:\n",
    "        train_channels = get_channels(train_index, task_channels)\n",
    "        test_channels = get_channels(test_index, task_channels)\n",
    "        channels = train_channels[0] + test_channels[0]\n",
    "    elif load_mode == 1:\n",
    "        train_channels = get_channels(train_index, task_channels)\n",
    "        val_channels = get_channels(val_index, task_channels)\n",
    "        test_channels = get_channels(test_index, task_channels)\n",
    "        channels = train_channels[0] + val_channels[0] + test_channels[0]\n",
    "\n",
    "    print(f\"Class: {class_name}\")\n",
    "    print(f\"Task: {task_name}\")\n",
    "\n",
    "    with open(f\"/home/maxihuber/eeg-foundation/src/finetune/v2/channels_per_task/{class_name}_{task_name}_channels.json\", \"w\") as f:\n",
    "        json.dump(channels, f)\n",
    "\n",
    "    channels_intersect = list(natsorted(list(set(channels) & set(task_channels))))\n",
    "\n",
    "    with open(f\"/home/maxihuber/eeg-foundation/src/finetune/v2/channels_per_task/{class_name}_{task_name}_channels_intersect.json\", \"w\") as f:\n",
    "        json.dump(channels_intersect, f)\n",
    "\n",
    "    #################################################################\n",
    "\n",
    "for class_name, [used_class, used_tasks] in classes.items():\n",
    "    for used_task in used_tasks:\n",
    "        class_name = used_class[\"class_name\"]\n",
    "        time_col = used_class[\"time_col\"]\n",
    "        prefix_filepath = used_class[\"prefix_filepath\"]\n",
    "        load_mode = used_class[\"load_mode\"]\n",
    "        task_name = used_task[\"task_name\"]\n",
    "        task_type = used_task[\"task_type\"]\n",
    "        json_path = used_task[\"json_path\"]\n",
    "        out_dim = used_task[\"out_dim\"]\n",
    "        short_mode = used_task[\"short_mode\"] if \"short_mode\" in used_task else False\n",
    "\n",
    "        print(f\"Getting data sizes for {class_name} - {task_name}\")\n",
    "\n",
    "        if load_mode != 1:\n",
    "            train_index, test_index = load_index0(json_path)\n",
    "            print(f\"Train size: {format_size(get_size(train_index))}\")\n",
    "            print(f\"Test size: {format_size(get_size(test_index))}\")\n",
    "        else:\n",
    "            train_index, val_index, test_index = load_index1(json_path)\n",
    "            print(f\"Train size: {format_size(get_size(train_index))}\")\n",
    "            print(f\"Val size: {format_size(get_size(val_index))}\")\n",
    "            print(f\"Test size: {format_size(get_size(test_index))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_data(data_index, task_channels):\n",
    "    num_samples = 0\n",
    "    data = {}\n",
    "    outputs = {}\n",
    "    srs = {}\n",
    "    durs = {}\n",
    "    channels = {}\n",
    "    datasets = {}\n",
    "    failed_samples = []\n",
    "\n",
    "    for sample in tqdm(data_index, desc=\"Loading data\", position=0, leave=True):\n",
    "        try:\n",
    "            # Load the data of this sample\n",
    "            input_files = get_full_paths(sample[\"input\"])\n",
    "\n",
    "            if load_mode == 2:\n",
    "                file = input_files[0]\n",
    "                df = load_edf_to_dataframe(file)\n",
    "            else:\n",
    "                dataframes = [pd.read_pickle(file) for file in input_files]\n",
    "                df = pd.concat(dataframes, axis=0)\n",
    "            \n",
    "            # Crop the data to the desired length\n",
    "            start = int(sample[\"start\"])\n",
    "            length = int(sample[\"length\"]) if \"length\" in sample else int(sample[\"end\"])\n",
    "            df.loc[start : start + length, :] if load_mode==1 else df.iloc[start:length, :]\n",
    "            assert len(df) > 0, f\"Empty dataframe for sample: {sample}\"\n",
    "   \n",
    "            # Add labels\n",
    "            if load_mode != 1:\n",
    "                outputs[num_samples] = sample.get(\"output\", sample.get(\"label\"))\n",
    "            else:\n",
    "                outputs[num_samples] = list(sample[\"output\"].values()) if task_name == \"EyeNetPosition\" else list(sample[\"output\"].values())[0]\n",
    "            \n",
    "            # Add metadata\n",
    "            sr = int(1 / (df[time_col].iloc[1] - df[time_col].iloc[0]))\n",
    "            srs[num_samples] = sr\n",
    "            durs[num_samples] = len(df) / sr\n",
    "            \n",
    "            channels[num_samples] = sorted(list(set(df.columns) & set(task_channels)), key=lambda x: list(task_channels).index(x))\n",
    "            df = df[channels[num_samples]].astype(float)\n",
    "            data[num_samples] = torch.tensor(df.to_numpy(), dtype=torch.float32).T\n",
    "\n",
    "            num_samples += 1\n",
    "\n",
    "            del df\n",
    "            if num_samples % 100 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process sample: {sample}. Error: {e}\", file=sys.stderr)\n",
    "            failed_samples.append(sample)\n",
    "\n",
    "    return data, outputs, srs, durs, channels\n",
    "\n",
    "used_class = yc_class\n",
    "used_task = tuab\n",
    "\n",
    "class_name = used_class[\"class_name\"]\n",
    "time_col = used_class[\"time_col\"]\n",
    "prefix_filepath = used_class[\"prefix_filepath\"]\n",
    "load_mode = used_class[\"load_mode\"]\n",
    "task_name = used_task[\"task_name\"]\n",
    "task_type = used_task[\"task_type\"]\n",
    "json_path = used_task[\"json_path\"]\n",
    "out_dim = used_task[\"out_dim\"]\n",
    "short_mode = used_task[\"short_mode\"] if \"short_mode\" in used_task else False\n",
    "\n",
    "if load_mode != 1:\n",
    "    train_index, test_index = load_index0(json_path)\n",
    "else:\n",
    "    train_index, val_index, test_index = load_index1(json_path)\n",
    "\n",
    "if load_mode == 0 or load_mode == 2:\n",
    "    print(\"=\" * 10 + \"Load train data\" + \"=\" * 100)\n",
    "    train_data, train_outputs, train_sr, train_dur, train_channels, train_datasets = (\n",
    "        load_file_data(train_index, task_channels)\n",
    "    )\n",
    "    print(\"=\" * 10 + \"Load test data\" + \"=\" * 100)\n",
    "    test_data, test_outputs, test_sr, test_dur, test_channels, test_datasets = (\n",
    "        load_file_data(test_index, task_channels)\n",
    "    )\n",
    "elif load_mode == 1:\n",
    "    train_data, train_outputs, train_sr, train_dur, train_channels, train_datasets = (\n",
    "        load_file_data(train_index, task_channels)\n",
    "    )\n",
    "    val_data, val_outputs, val_sr, val_dur, val_channels, val_datasets = load_file_data(\n",
    "        val_index, task_channels\n",
    "    )\n",
    "    test_data, test_outputs, test_sr, test_dur, test_channels, test_datasets = (\n",
    "        load_file_data(test_index, task_channels)\n",
    "    )\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shorten Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting output values for Clinical - Age\n",
      "Originally had 565 train and 139 test indices.\n",
      "Downsampled to 91 (49.92 GB) train and 106 (49.76 GB) test indices.\n",
      "Dumped to /itet-stor/maxihuber/net_scratch/finetune_files/age2_light.json.\n",
      "====================================================================================================\n",
      "Getting output values for Clinical - Depression\n",
      "Originally had 206 train and 33 test indices.\n",
      "Output: No Depression - Train-size: 170 | Test-size: 24\n",
      "Output: Depression - Train-size: 36 | Test-size: 9\n",
      "Downsampled to 158 (49.98 GB) train and 33 (10.6 GB) test indices.\n",
      "Output: No Depression - Train-size: 122 | Test-size: 24\n",
      "Output: Depression - Train-size: 36 | Test-size: 9\n",
      "Dumped to /itet-stor/maxihuber/net_scratch/finetune_files/cli_depression_light.json.\n",
      "====================================================================================================\n",
      "Getting output values for Clinical - Parkinsons\n",
      "Originally had 363 train and 129 test indices.\n",
      "Output: PD - Train-size: 250 | Test-size: 78\n",
      "Output: No PD - Train-size: 113 | Test-size: 51\n",
      "Downsampled to 77 (50.0 GB) train and 129 (49.84 GB) test indices.\n",
      "Output: PD - Train-size: 40 | Test-size: 78\n",
      "Output: No PD - Train-size: 37 | Test-size: 51\n",
      "Dumped to /itet-stor/maxihuber/net_scratch/finetune_files/parkinsons2_light.json.\n",
      "====================================================================================================\n",
      "Getting output values for Clinical - Schizophrenia\n",
      "Originally had 69 train and 7 test indices.\n",
      "Output: No Schizophrenia - Train-size: 27 | Test-size: 4\n",
      "Output: Schizophrenia - Train-size: 42 | Test-size: 3\n",
      "Downsampled to 37 (49.5 GB) train and 7 (9.29 GB) test indices.\n",
      "Output: No Schizophrenia - Train-size: 19 | Test-size: 4\n",
      "Output: Schizophrenia - Train-size: 18 | Test-size: 3\n",
      "Dumped to /itet-stor/maxihuber/net_scratch/finetune_files/cli_schizophrenia_light.json.\n",
      "====================================================================================================\n",
      "Getting output values for Clinical - Sex\n",
      "Originally had 561 train and 137 test indices.\n",
      "Output: M - Train-size: 306 | Test-size: 76\n",
      "Output: F - Train-size: 254 | Test-size: 61\n",
      "Output:  - Train-size: 1 | Test-size: 0\n",
      "Downsampled to 74 (49.99 GB) train and 118 (49.92 GB) test indices.\n",
      "Output: M - Train-size: 38 | Test-size: 59\n",
      "Output: F - Train-size: 35 | Test-size: 59\n",
      "Output:  - Train-size: 1 | Test-size: 0\n",
      "Dumped to /itet-stor/maxihuber/net_scratch/finetune_files/sex2_light.json.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def get_sample_size(sample, prefix_filepath):\n",
    "        input_files = get_full_paths(sample[\"input\"], prefix_filepath)\n",
    "        sz = sum([os.path.getsize(file) for file in input_files])\n",
    "        return sz\n",
    "\n",
    "def sample_data(outputs, index, max_size):\n",
    "    \"\"\"Sample data indices based on class balance or randomly, respecting a maximum size limit.\"\"\"\n",
    "    selected_indices = []\n",
    "    current_size = 0\n",
    "    if task_type == \"Classification\":\n",
    "        # We shuffle the outputs for each class to ensure random selection within each class\n",
    "        for output, indices in outputs.items():\n",
    "            random.shuffle(indices)\n",
    "        all_outputs = list(outputs.keys())\n",
    "        while current_size < max_size and all_outputs:\n",
    "            for output in all_outputs[:]:\n",
    "                if outputs[output]:\n",
    "                    idx = outputs[output].pop(0)\n",
    "                    size = get_sample_size(index[idx], prefix_filepath)\n",
    "                    if current_size + size <= max_size:\n",
    "                        selected_indices.append(idx)\n",
    "                        current_size += size\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    all_outputs.remove(output)\n",
    "    else:  # Regression\n",
    "        all_indices = [idx for indices in outputs.values() for idx in indices]\n",
    "        random.shuffle(all_indices)\n",
    "        for idx in all_indices:\n",
    "            size = get_sample_size(index[idx], prefix_filepath)\n",
    "            if current_size + size <= max_size:\n",
    "                selected_indices.append(idx)\n",
    "                current_size += size\n",
    "            else:\n",
    "                break\n",
    "    return selected_indices\n",
    "\n",
    "classes_to_make_light = {\n",
    "    # 'YC': [yc_class, yc_tasks],\n",
    "    'Clinical': [cli_class, cli_tasks],\n",
    "    # 'MI': [mi_class, mi_tasks],\n",
    "    # 'ERP': [erp_class, erp_tasks],\n",
    "}\n",
    "\n",
    "for class_name, [used_class, used_tasks] in classes_to_make_light.items():\n",
    "    for used_task in used_tasks:\n",
    "        class_name = used_class[\"class_name\"]\n",
    "        time_col = used_class[\"time_col\"]\n",
    "        prefix_filepath = used_class[\"prefix_filepath\"]\n",
    "        load_mode = used_class[\"load_mode\"]\n",
    "        task_name = used_task[\"task_name\"]\n",
    "        task_type = used_task[\"task_type\"]\n",
    "        json_path = used_task[\"json_path\"]\n",
    "        out_dim = used_task[\"out_dim\"]\n",
    "        short_mode = used_task[\"short_mode\"] if \"short_mode\" in used_task else False\n",
    "\n",
    "        print(f\"Getting output values for {class_name} - {task_name}\")\n",
    "\n",
    "        if load_mode != 1:\n",
    "            train_index, test_index = load_index0(json_path)\n",
    "        else:\n",
    "            train_index, val_index, test_index = load_index1(json_path)\n",
    "        \n",
    "        # Get output classes and their indices in train_index\n",
    "        def get_outputs(data_index, load_mode):\n",
    "            outputs = {}\n",
    "            for i, sample in enumerate(data_index):\n",
    "                output_val = sample.get(\"output\", sample.get(\"label\")) if load_mode != 1 else list(sample[\"output\"].values()) if task_name == \"EyeNetPosition\" else list(sample[\"output\"].values())[0]\n",
    "                if output_val in outputs:\n",
    "                    outputs[output_val].append(i)\n",
    "                else:\n",
    "                    outputs[output_val] = [i]\n",
    "            return outputs\n",
    "\n",
    "        def print_output_distribution(train_outputs, test_outputs, task_type):\n",
    "            if task_type == \"Classification\":\n",
    "                for output in train_outputs.keys():\n",
    "                    try:\n",
    "                        print(f\"Output: {output} - Train-size: {len(train_outputs[output])} | Test-size: {len(test_outputs[output])}\")\n",
    "                    except KeyError:\n",
    "                        print(f\"Output: {output} - Train-size: {len(train_outputs[output])} | Test-size: 0\")\n",
    "\n",
    "        print(f\"Originally had {len(train_index)} train and {len(test_index)} test indices.\")\n",
    "        train_outputs = get_outputs(train_index, load_mode)\n",
    "        test_outputs = get_outputs(test_index, load_mode)\n",
    "        # k1 = 'with event-related potential' if task_name == 'ERP' else 'with error-related potential'\n",
    "        # k2 = 'without event-related potential' if task_name == 'ERP' else 'without error-related potential'\n",
    "        # train_outputs = {k1: train_outputs[k1], k2: train_outputs[k2]}\n",
    "        # test_outputs = {k1: test_outputs[k1], k2: test_outputs[k2]}\n",
    "        print_output_distribution(train_outputs, test_outputs, task_type)\n",
    "\n",
    "        # now, for classification, sample an element from each class at a time, until you have reached 50GB\n",
    "        #  -> this should ensure balanced classes, and if one class is exhausted, you stop\n",
    "        # for regression, sample randomly without replacement until you have reached 50GB (indices list, shuffled, pick until 50GB)\n",
    "        # to get the size of a sample, you can make use of the get_sample_size method\n",
    "        # train_new: <=50G, test_new: <=50GB\n",
    "        train_index_new = sample_data(train_outputs, train_index, 50 * 1024**3)  # 50GB\n",
    "        train_index_new = [train_index[i] for i in train_index_new]\n",
    "        test_index_new = sample_data(test_outputs, test_index, 50 * 1024**3)  # 50GB\n",
    "        test_index_new = [test_index[i] for i in test_index_new]\n",
    "\n",
    "        print(f\"Downsampled to {len(train_index_new)} ({format_size(get_index_size(train_index_new, prefix_filepath))}) train and {len(test_index_new)} ({format_size(get_index_size(test_index_new, prefix_filepath))}) test indices.\")\n",
    "        train_outputs_new = get_outputs(train_index_new, load_mode)\n",
    "        test_outputs_new = get_outputs(test_index_new, load_mode)\n",
    "        print_output_distribution(train_outputs_new, test_outputs_new, task_type)\n",
    "\n",
    "        # store\n",
    "        train_test_dict_light = {\"train\": train_index_new, \"test\": test_index_new}\n",
    "        path_prefix = '/itet-stor/maxihuber/net_scratch/finetune_files/'\n",
    "        filename_light = os.path.basename(json_path)[:-5] + \"_light.json\"\n",
    "        json_path_light = path_prefix + filename_light\n",
    "\n",
    "        with open(json_path_light, \"w\") as f:\n",
    "            json.dump(train_test_dict_light, f)\n",
    "            print(f\"Dumped to {json_path_light}!\")\n",
    "        \n",
    "        print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
